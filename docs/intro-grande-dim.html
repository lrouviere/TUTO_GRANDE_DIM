<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapitre 1 Introduction à la grande dimension | Régression en grande dimension</title>
  <meta name="description" content="Chapitre 1 Introduction à la grande dimension | Régression en grande dimension" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapitre 1 Introduction à la grande dimension | Régression en grande dimension" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapitre 1 Introduction à la grande dimension | Régression en grande dimension" />
  
  
  

<meta name="author" content="Laurent Rouvière" />


<meta name="date" content="2020-09-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="reg-comp.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Régression en grande dimension<br> <br> L. Rouvière</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Présentation</a></li>
<li class="chapter" data-level="1" data-path="intro-grande-dim.html"><a href="intro-grande-dim.html"><i class="fa fa-check"></i><b>1</b> Introduction à la grande dimension</a><ul>
<li class="chapter" data-level="1.1" data-path="intro-grande-dim.html"><a href="intro-grande-dim.html#fléau-de-la-dimension-pour-les-plus-proches-voisins"><i class="fa fa-check"></i><b>1.1</b> Fléau de la dimension pour les plus proches voisins</a></li>
<li class="chapter" data-level="1.2" data-path="intro-grande-dim.html"><a href="intro-grande-dim.html#influence-de-la-dimension-dans-le-modèle-linéaire"><i class="fa fa-check"></i><b>1.2</b> Influence de la dimension dans le modèle linéaire</a></li>
<li class="chapter" data-level="1.3" data-path="intro-grande-dim.html"><a href="intro-grande-dim.html#exercices"><i class="fa fa-check"></i><b>1.3</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="reg-comp.html"><a href="reg-comp.html"><i class="fa fa-check"></i><b>2</b> Régression sur composantes</a><ul>
<li class="chapter" data-level="2.1" data-path="reg-comp.html"><a href="reg-comp.html#sélection-de-variables"><i class="fa fa-check"></i><b>2.1</b> Sélection de variables</a></li>
<li class="chapter" data-level="2.2" data-path="reg-comp.html"><a href="reg-comp.html#régression-sur-composantes-principales-méthodo"><i class="fa fa-check"></i><b>2.2</b> Régression sur composantes principales (méthodo)</a></li>
<li class="chapter" data-level="2.3" data-path="reg-comp.html"><a href="reg-comp.html#régression-pls-méthodo"><i class="fa fa-check"></i><b>2.3</b> Régression PLS : méthodo</a></li>
<li class="chapter" data-level="2.4" data-path="reg-comp.html"><a href="reg-comp.html#comparaison-pcr-vs-pls."><i class="fa fa-check"></i><b>2.4</b> Comparaison : PCR vs PLS.</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="reg-pen.html"><a href="reg-pen.html"><i class="fa fa-check"></i><b>3</b> Régressions pénalisées (ou sous contraintes)</a><ul>
<li class="chapter" data-level="3.1" data-path="reg-pen.html"><a href="reg-pen.html#ridge-et-lasso-avec-glmnet"><i class="fa fa-check"></i><b>3.1</b> Ridge et lasso avec glmnet</a></li>
<li class="chapter" data-level="3.2" data-path="reg-pen.html"><a href="reg-pen.html#reconstruction-dun-signal"><i class="fa fa-check"></i><b>3.2</b> Reconstruction d’un signal</a></li>
<li class="chapter" data-level="3.3" data-path="reg-pen.html"><a href="reg-pen.html#régression-logistique-pénalisée"><i class="fa fa-check"></i><b>3.3</b> Régression logistique pénalisée</a></li>
<li class="chapter" data-level="3.4" data-path="reg-pen.html"><a href="reg-pen.html#exo-ridgelasso"><i class="fa fa-check"></i><b>3.4</b> Exercices</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="mod-add.html"><a href="mod-add.html"><i class="fa fa-check"></i><b>4</b> Modèle additif</a><ul>
<li class="chapter" data-level="4.1" data-path="mod-add.html"><a href="mod-add.html#pseudo-backfitting"><i class="fa fa-check"></i><b>4.1</b> Pseudo backfitting</a></li>
<li class="chapter" data-level="4.2" data-path="mod-add.html"><a href="mod-add.html#modèle-gam"><i class="fa fa-check"></i><b>4.2</b> Modèle GAM</a></li>
<li class="chapter" data-level="4.3" data-path="mod-add.html"><a href="mod-add.html#régression-logistique-additive"><i class="fa fa-check"></i><b>4.3</b> Régression logistique additive</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="références.html"><a href="références.html"><i class="fa fa-check"></i>Références</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Régression en grande dimension</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro-grande-dim" class="section level1">
<h1><span class="header-section-number">Chapitre 1</span> Introduction à la grande dimension</h1>
<p>Nous proposons ici d’illustrer le problème de la grande dimension en régression. On commencera par étudier, à l’aide de simulation, ce problème pour l’estimateurs des <span class="math inline">\(k\)</span> plus proches voisins, puis pour les estimateurs des moindres carrés dans le modèle linéaire. Quelques exercices sont ensuite proposées pour calculer les vitesses de convergence de ces estimateurs dans des modèles simples.</p>
<div id="fléau-de-la-dimension-pour-les-plus-proches-voisins" class="section level2">
<h2><span class="header-section-number">1.1</span> Fléau de la dimension pour les plus proches voisins</h2>
<p>La fonction suivante permet de générer un échantillon d’apprentissage et un échantillon test selon le modèle
<span class="math display">\[Y=X_1^2+\dots+X_p^2+\varepsilon\]</span>
où les <span class="math inline">\(X_j\)</span> sont uniformes i.i.d de loi uniorme sur <span class="math inline">\([0,1]\)</span> et le bruit <span class="math inline">\(\varepsilon\)</span> suit une loi <span class="math inline">\(\mathcal N(0,0.5^2)\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="intro-grande-dim.html#cb1-1"></a>simu &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">napp=</span><span class="dv">300</span>,<span class="dt">ntest=</span><span class="dv">500</span>,<span class="dt">p=</span><span class="dv">3</span>,<span class="dt">graine=</span><span class="dv">1234</span>){</span>
<span id="cb1-2"><a href="intro-grande-dim.html#cb1-2"></a>  <span class="kw">set.seed</span>(graine)</span>
<span id="cb1-3"><a href="intro-grande-dim.html#cb1-3"></a>  n &lt;-<span class="st"> </span>napp<span class="op">+</span>ntest</span>
<span id="cb1-4"><a href="intro-grande-dim.html#cb1-4"></a>  X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>p),<span class="dt">ncol=</span>p)</span>
<span id="cb1-5"><a href="intro-grande-dim.html#cb1-5"></a>  Y &lt;-<span class="st"> </span><span class="kw">apply</span>(X<span class="op">^</span><span class="dv">2</span>,<span class="dv">1</span>,sum)<span class="op">+</span><span class="kw">rnorm</span>(n,<span class="dt">sd=</span><span class="fl">0.5</span>)</span>
<span id="cb1-6"><a href="intro-grande-dim.html#cb1-6"></a>  Yapp &lt;-<span class="st"> </span>Y[<span class="dv">1</span><span class="op">:</span>napp]</span>
<span id="cb1-7"><a href="intro-grande-dim.html#cb1-7"></a>  Ytest &lt;-<span class="st"> </span>Y[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span>napp)]</span>
<span id="cb1-8"><a href="intro-grande-dim.html#cb1-8"></a>  Xapp &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X[<span class="dv">1</span><span class="op">:</span>napp,])</span>
<span id="cb1-9"><a href="intro-grande-dim.html#cb1-9"></a>  Xtest &lt;-<span class="st"> </span><span class="kw">data.frame</span>(X[<span class="op">-</span>(<span class="dv">1</span><span class="op">:</span>napp),])</span>
<span id="cb1-10"><a href="intro-grande-dim.html#cb1-10"></a>  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">Xapp=</span>Xapp,<span class="dt">Yapp=</span>Yapp,<span class="dt">Xtest=</span>Xtest,<span class="dt">Ytest=</span>Ytest))</span>
<span id="cb1-11"><a href="intro-grande-dim.html#cb1-11"></a>}</span>
<span id="cb1-12"><a href="intro-grande-dim.html#cb1-12"></a>df &lt;-<span class="st"> </span><span class="kw">simu</span>(<span class="dt">napp=</span><span class="dv">300</span>,<span class="dt">ntest=</span><span class="dv">500</span>,<span class="dt">p=</span><span class="dv">3</span>,<span class="dt">graine=</span><span class="dv">1234</span>)</span></code></pre></div>
<p>La fonction <strong>knn.reg</strong> du package <strong>FNN</strong> permet de construire des estimateurs des <span class="math inline">\(k\)</span> plus proches voisins en régression. On peut par exemple faire du 3 plus proches voisins avec</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="intro-grande-dim.html#cb2-1"></a><span class="kw">library</span>(FNN)</span>
<span id="cb2-2"><a href="intro-grande-dim.html#cb2-2"></a>mod3ppv &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="dt">train=</span>df<span class="op">$</span>Xapp,<span class="dt">y=</span>df<span class="op">$</span>Yapp,<span class="dt">k=</span><span class="dv">3</span>)</span></code></pre></div>
<p>Parmi toutes les sorties proposées par cette fonction on a notamment</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="intro-grande-dim.html#cb3-1"></a>mod3ppv<span class="op">$</span>PRESS</span>
<span id="cb3-2"><a href="intro-grande-dim.html#cb3-2"></a>[<span class="dv">1</span>] <span class="fl">98.98178</span></span></code></pre></div>
<p>qui renvoie la somme des carrés des erreurs de prévision par validation croisée Leave-One-Out (LOO). On peut ainsi obtenir l’erreur quadratique moyenne par LOO</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="intro-grande-dim.html#cb4-1"></a>mod3ppv<span class="op">$</span>PRESS<span class="op">/</span><span class="kw">max</span>(<span class="kw">c</span>(<span class="kw">nrow</span>(df<span class="op">$</span>Xapp),<span class="dv">1</span>))</span>
<span id="cb4-2"><a href="intro-grande-dim.html#cb4-2"></a>[<span class="dv">1</span>] <span class="fl">0.3299393</span></span></code></pre></div>
<ol style="list-style-type: decimal">
<li><p>Construire la fonction <strong>sel.k</strong> qui admet en entrée :</p>
<ul>
<li>une grille de valeurs possibles de plus proches voisins (un vecteur).</li>
<li>une matrice <strong>Xapp</strong> de dimension <span class="math inline">\(n\times p\)</span> qui contient les valeurs variables explicatives.</li>
<li>un vecteur <strong>Yapp</strong> de dimension <span class="math inline">\(n\)</span> qui contient les valeurs de la variable à expliquer</li>
</ul>
<p>et qui renvoie en sortie la valeur de <span class="math inline">\(k\)</span> dans la grille qui minimise l’erreur LOO présentée ci-dessus.</p>
<p>Une fois la fonction créée, on peut calculer l’erreur de l’estimateur sélectionné sur un échantillon test avec</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="intro-grande-dim.html#cb5-1"></a>k.opt &lt;-<span class="st"> </span><span class="kw">sel.k</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">50</span>,<span class="dt">by=</span><span class="dv">5</span>),df<span class="op">$</span>Xapp,df<span class="op">$</span>Yapp)</span>
<span id="cb5-2"><a href="intro-grande-dim.html#cb5-2"></a>prev &lt;-<span class="st"> </span><span class="kw">knn.reg</span>(<span class="dt">train=</span>df<span class="op">$</span>Xapp,<span class="dt">y=</span>df<span class="op">$</span>Yapp,<span class="dt">test=</span>df<span class="op">$</span>Xtest,<span class="dt">k=</span>k.opt)<span class="op">$</span>pred</span>
<span id="cb5-3"><a href="intro-grande-dim.html#cb5-3"></a><span class="kw">mean</span>((prev<span class="op">-</span>df<span class="op">$</span>Ytest)<span class="op">^</span><span class="dv">2</span>)</span></code></pre></div></li>
<li><p>On souhaite comparer les erreurs des règles des <span class="math inline">\(k\)</span> plus proches voisins en fonction de la dimension. On considère 4 dimensions collectées dans le vecteur <code>DIM</code> et la grille de valeurs de <span class="math inline">\(k\)</span> suivantes :</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="intro-grande-dim.html#cb6-1"></a>DIM &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">50</span>)</span>
<span id="cb6-2"><a href="intro-grande-dim.html#cb6-2"></a>K_cand &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">50</span>,<span class="dt">by=</span><span class="dv">5</span>)</span></code></pre></div>
<p>Pour chaque valeur de dimension répéter <span class="math inline">\(B=100\)</span> fois :</p>
<ul>
<li>simuler un échantillon d’apprentissage de taille 300 et test de taille 500</li>
<li>calculer la valeur optimale de <span class="math inline">\(k\)</span> dans <strong>K_cand</strong> grâce à <strong>sel.k</strong></li>
<li>calculer l’erreur de l’estimateur sélectionné sur un échantillon test.</li>
</ul>
<p>On pourra stocker les résultats dans une matrice de dimension <span class="math inline">\(B\times 4\)</span>.</p></li>
<li><p>A l’aide d’indicateurs numériques et de boxplots, comparer la distribution des erreurs en fonction de la dimension.</p></li>
<li><p>Conclure</p></li>
</ol>
</div>
<div id="influence-de-la-dimension-dans-le-modèle-linéaire" class="section level2">
<h2><span class="header-section-number">1.2</span> Influence de la dimension dans le modèle linéaire</h2>
<p>En vous basant sur l’exercice précédent, proposer une illustration qui peut mettre en évidence la précision d’estimation dans le modèle linéaire en fonction de la dimension. On pourra par exemple considérer le modèle linaire suivant
<span class="math display">\[Y=X_1+0X_2+\dots+0X_p+\varepsilon\]</span>
et étudier la performance de l’estimateur MCO du coefficient de <span class="math inline">\(X_1\)</span> pour différentes valeurs de <span class="math inline">\(p\)</span>. Par exemple avec <span class="math inline">\(p\)</span> dans le vecteur</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="intro-grande-dim.html#cb7-1"></a>DIM &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">50</span>,<span class="dv">100</span>,<span class="dv">200</span>)</span></code></pre></div>
<p>Les données pourront être générées avec la fonction suivante</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="intro-grande-dim.html#cb8-1"></a>n &lt;-<span class="st"> </span><span class="dv">250</span></span>
<span id="cb8-2"><a href="intro-grande-dim.html#cb8-2"></a>p &lt;-<span class="st"> </span><span class="dv">1000</span></span>
<span id="cb8-3"><a href="intro-grande-dim.html#cb8-3"></a>X &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n<span class="op">*</span>p),<span class="dt">ncol=</span>p)</span>
<span id="cb8-4"><a href="intro-grande-dim.html#cb8-4"></a>simu.lin &lt;-<span class="st"> </span><span class="cf">function</span>(X,graine){</span>
<span id="cb8-5"><a href="intro-grande-dim.html#cb8-5"></a>  <span class="kw">set.seed</span>(graine)</span>
<span id="cb8-6"><a href="intro-grande-dim.html#cb8-6"></a>  Y &lt;-<span class="st"> </span>X[,<span class="dv">1</span>]<span class="op">+</span><span class="kw">rnorm</span>(<span class="kw">nrow</span>(X),<span class="dt">sd=</span><span class="fl">0.5</span>)</span>
<span id="cb8-7"><a href="intro-grande-dim.html#cb8-7"></a>  df &lt;-<span class="st"> </span><span class="kw">data.frame</span>(Y,X)</span>
<span id="cb8-8"><a href="intro-grande-dim.html#cb8-8"></a>  <span class="kw">return</span>(df)</span>
<span id="cb8-9"><a href="intro-grande-dim.html#cb8-9"></a>}</span></code></pre></div>
</div>
<div id="exercices" class="section level2">
<h2><span class="header-section-number">1.3</span> Exercices</h2>

<div class="exercise">
<span id="exr:exo-dist-points" class="exercise"><strong>Exercice 1.1  (Distances entre deux points, voir <span class="citation">Giraud (<a href="#ref-gir15" role="doc-biblioref">2015</a>)</span>)  </strong></span>
</div>

<p>Soit <span class="math inline">\(X^{(1)}=(X_1^{(1)},\dots,X_p^{(1)})\)</span> et <span class="math inline">\(X^{(2)}=(X_1^{(2)},\dots,X_p^{(2)})\)</span> deux variables aléatoires indépendantes de loi uniforme sur l’hypercube <span class="math inline">\([0,1]^p\)</span>. Montrer que
<span class="math display">\[\mathbf E[\|X^{(1)}-X^{(2)}\|^2]=\frac{p}{6}\quad\text{et}\quad\sigma[\|X^{(1)}-X^{(2)}\|^2]\approx 0.2\sqrt{p}.\]</span></p>

<div class="exercise">
<span id="exr:exo-noyau-curse" class="exercise"><strong>Exercice 1.2  (Vitesse de convergence pour l’estimateur à noyau)  </strong></span>
</div>

<p>On considère le modèle de régression
<span class="math display">\[Y_i=m(x_i)+\varepsilon_i,\quad i=1,\dots,n\]</span>
où <span class="math inline">\(x_1,\dots,x_n\in\mathbb R^d\)</span> sont déterministes et <span class="math inline">\(\varepsilon_1,\dots,\varepsilon_n\)</span> sont des variables i.i.d. d’espérance nulle et de variance <span class="math inline">\(\sigma^2&lt;+\infty\)</span>. On désigne par <span class="math inline">\(\|\,.\,\|\)</span> la norme Euclidienne dans <span class="math inline">\(\mathbb R^d\)</span>. On définit l’estimateur localement constant de <span class="math inline">\(m\)</span> en <span class="math inline">\(x\in\mathbb R^d\)</span> par :
<span class="math display">\[\hat m(x)=\mathop{\mathrm{argmin}}_{a\in\mathbb R}\sum_{i=1}^n(Y_i-a)^2K\left(\frac{\|x_i-x\|}{h}\right)\]</span>
où <span class="math inline">\(h&gt;0\)</span> et pour <span class="math inline">\(u\in\mathbb R,K(u)=\mathbf 1_{[0,1]}(u)\)</span>. On suppose que <span class="math inline">\(\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)&gt;0\)</span>.</p>
<ol style="list-style-type: decimal">
<li><p>Donner la forme explicite de <span class="math inline">\(\hat m(x)\)</span>.</p></li>
<li><p>Montrer que
<span class="math display">\[\mathbf V[\hat m(x)]= \frac{\sigma^2}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}\]</span>
et
<span class="math display">\[\mathbf E[\hat m(x)]-m(x)=\frac{\sum_{i=1}^n(m(x_i)-m(x))K\left(\frac{\|x_i-x\|}{h}\right)}{\sum_{i=1}^nK\left(\frac{\|x_i-x\|}{h}\right)}.\]</span></p></li>
<li><p>On suppose maintenant que <span class="math inline">\(m\)</span> est Lipschitzienne de constante <span class="math inline">\(L\)</span>, c’est-à-dire que <span class="math inline">\(\forall (x_1,x_2)\in\mathbb R^d\times\mathbb R^d\)</span>
<span class="math display">\[|m(x_1)-m(x_2)|\leq L\|x_1-x_2\|.\]</span>
Montrer que
<span class="math display">\[|\textrm{biais}[\hat m(x)]|\leq Lh.\]</span></p></li>
<li><p>On suppose de plus qu’il existe une constante <span class="math inline">\(C_1\)</span> telle que
<span class="math display">\[C_1\leq\frac{\sum_{i=1}^n\mathbf 1_{B_h}(x_i-x)}{n\textrm{Vol}(B_h)},\]</span>
où <span class="math inline">\(B_h=\{u\in\mathbb R^d:\|u\|\leq h\}\)</span> est la boule de rayon <span class="math inline">\(h\)</span> dans <span class="math inline">\(\mathbb R^d\)</span> et <span class="math inline">\(\textrm{Vol}(A)\)</span> désigne le volume d’un ensemble <span class="math inline">\(A\subset\mathbb R^d\)</span>. Montrer que
<span class="math display">\[\mathbf V[\hat m(x)]\leq\frac{C_2\sigma^2}{nh^d},\]</span>
où <span class="math inline">\(C_2\)</span> est une constante dépendant de <span class="math inline">\(C_1\)</span> et <span class="math inline">\(d\)</span> à préciser.</p></li>
<li><p>Déduire des questions précédentes un majorant de l’erreur quadratique moyenne de <span class="math inline">\(\hat m(x)\)</span>.</p></li>
<li><p>Calculer <span class="math inline">\(h_{\text{opt}}\)</span> la valeur de <span class="math inline">\(h\)</span> qui minimise ce majorant. Que vaut ce majorant lorsque <span class="math inline">\(h=h_{\text{opt}}\)</span> ? Comment varie cette vitesse lorsque <span class="math inline">\(d\)</span> augmente ? Interpréter.</p></li>
</ol>

</div>
</div>
<h3>Références</h3>
<div id="refs" class="references">
<div id="ref-gir15">
<p>Giraud, C. 2015. <em>Introduction to High-Dimensional Statistics</em>. CRC Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reg-comp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["TUTO_GRANDE_DIM.pdf"],
"toc": {
"collapse": "subsection",
"fontsettings": {
"theme": "white",
"size": 2,
"family": "sans"
},
"sharing": {
"facebook": true,
"github": true,
"twitter": true
}
},
"highlight": "default"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistique en grande dimension",
    "section": "",
    "text": "Ce tutoriel présente quelques exercices d’application du cours Modèle linéaire en grande dimension. On pourra trouver\n\nles supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/stat_grand_dim/ ;\nle tutoriel sans les corrections à l’url https://lrouviere.github.io/TUTO_GRANDE_DIM/\nle tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_GRANDE_DIM/correction.\n\nIl est recommandé d’utiliser mozilla firefox pour lire le tutoriel.\nDes connaissances de base en R et en statistique (modèles de régression) sont nécessaires. Le tutoriel se structure en 4 parties :\n\nFléau de la dimension : identification du problème de la dimension pour le problème de régression ;\nRégression sur composantes : présentation des algorithmes PCR et PLS ;\nRégressions pénalisées: régularisation à l’aide de pénalités de type Ridge/Lasso\nModèle additif : conservation de la structure additive du modèle linéaire mais modélisation non paramétrique des composantes."
  },
  {
    "objectID": "01-intro-grande-dim.html",
    "href": "01-intro-grande-dim.html",
    "title": "1  Les problèmes de la grande dimension",
    "section": "",
    "text": "Nous proposons ici d’illustrer le problème de la grande dimension en régression. On commencera par étudier, à l’aide de simulation, ce problème pour l’estimateur des \\(k\\) plus proches voisins, puis pour les estimateurs des moindres carrés dans le modèle linéaire. Quelques exercices sont ensuite proposées pour calculer les vitesses de convergence de ces estimateurs dans des modèles simples."
  },
  {
    "objectID": "01-intro-grande-dim.html#fléau-de-la-dimension-pour-les-plus-proches-voisins",
    "href": "01-intro-grande-dim.html#fléau-de-la-dimension-pour-les-plus-proches-voisins",
    "title": "1  Les problèmes de la grande dimension",
    "section": "1.1 Fléau de la dimension pour les plus proches voisins",
    "text": "1.1 Fléau de la dimension pour les plus proches voisins\nLa fonction suivante permet de générer un échantillon d’apprentissage et un échantillon test selon le modèle \\[Y=X_1^2+\\dots+X_p^2+\\varepsilon\\] où les \\(X_j\\) sont uniformes i.i.d de loi uniorme sur \\([0,1]\\) et le bruit \\(\\varepsilon\\) suit une loi \\(\\mathcal N(0,0.5^2)\\).\n\nsimu <- function(napp=300,ntest=500,p=3,graine=1234){\n  set.seed(graine)\n  n <- napp+ntest\n  X <- matrix(runif(n*p),ncol=p)\n  Y <- apply(X^2,1,sum)+rnorm(n,sd=0.5)\n  Yapp <- Y[1:napp]\n  Ytest <- Y[-(1:napp)]\n  Xapp <- data.frame(X[1:napp,])\n  Xtest <- data.frame(X[-(1:napp),])\n  return(list(Xapp=Xapp,Yapp=Yapp,Xtest=Xtest,Ytest=Ytest))\n}\ndf <- simu(napp=300,ntest=500,p=3,graine=1234)\n\nLa fonction knn.reg du package FNN permet de construire des estimateurs des \\(k\\) plus proches voisins en régression. On peut par exemple faire du 3 plus proches voisins avec\n\nlibrary(FNN)\nmod3ppv <- knn.reg(train=df$Xapp,y=df$Yapp,k=3)\n\nParmi toutes les sorties proposées par cette fonction on a notamment\n\nmod3ppv$PRESS\n\n[1] 98.98178\n\n\nqui renvoie la somme des carrés des erreurs de prévision par validation croisée Leave-One-Out (LOO). On peut ainsi obtenir l’erreur quadratique moyenne par LOO\n\nmod3ppv$PRESS/max(c(nrow(df$Xapp),1))\n\n[1] 0.3299393\n\n\n\nConstruire la fonction sel.k qui admet en entrée :\n\nune grille de valeurs possibles de plus proches voisins (un vecteur).\nune matrice Xapp de dimension \\(n\\times p\\) qui contient les valeurs variables explicatives.\nun vecteur Yapp de dimension \\(n\\) qui contient les valeurs de la variable à expliquer\n\net qui renvoie en sortie la valeur de \\(k\\) dans la grille qui minimise l’erreur LOO présentée ci-dessus.\n\n\n\nUne fois la fonction créée, on peut calculer l’erreur de l’estimateur sélectionné sur un échantillon test avec\n\nk.opt <- sel.k(seq(1,50,by=5),df$Xapp,df$Yapp)\nprev <- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred\nmean((prev-df$Ytest)^2)\n\nOn souhaite comparer les erreurs des règles des \\(k\\) plus proches voisins en fonction de la dimension. On considère 4 dimensions collectées dans le vecteur DIM et la grille de valeurs de \\(k\\) suivantes :\n\nDIM <- c(1,5,10,50)\nK_cand <- seq(1,50,by=5)\n\nPour chaque valeur de dimension répéter \\(B=100\\) fois :\n\nsimuler un échantillon d’apprentissage de taille 300 et test de taille 500\ncalculer la valeur optimale de \\(k\\) dans K_cand grâce à sel.k\ncalculer l’erreur de l’estimateur sélectionné sur un échantillon test.\n\nOn pourra stocker les résultats dans une matrice de dimension \\(B\\times 4\\).\n\n\n\nA l’aide d’indicateurs numériques et de boxplots, comparer la distribution des erreurs en fonction de la dimension.\n\n\n\n\n\n\n\n\n\nConclure\n\nLes estimateurs sont moins précis lorsque la dimension augmente. C’est le fléau de la dimension."
  },
  {
    "objectID": "01-intro-grande-dim.html#influence-de-la-dimension-dans-le-modèle-linéaire",
    "href": "01-intro-grande-dim.html#influence-de-la-dimension-dans-le-modèle-linéaire",
    "title": "1  Les problèmes de la grande dimension",
    "section": "1.2 Influence de la dimension dans le modèle linéaire",
    "text": "1.2 Influence de la dimension dans le modèle linéaire\nEn vous basant sur l’exercice précédent, proposer une illustration qui peut mettre en évidence la précision d’estimation dans le modèle linéaire en fonction de la dimension. On pourra par exemple considérer le modèle linaire suivant \\[Y=X_1+0X_2+\\dots+0X_p+\\varepsilon\\] et étudier la performance de l’estimateur MCO du coefficient de \\(X_1\\) pour différentes valeurs de \\(p\\). Par exemple avec \\(p\\) dans le vecteur\n\nDIM <- c(0,50,100,200)\n\nLes données pourront être générées avec la fonction suivante\n\nn <- 250\np <- 1000\nX <- matrix(runif(n*p),ncol=p)\nsimu.lin <- function(X,graine){\n  set.seed(graine)\n  Y <- X[,1]+rnorm(nrow(X),sd=0.5)\n  df <- data.frame(Y,X)\n  return(df)\n}\n\n\nOn s’intéresse à la distribution de \\(\\widehat\\beta_1\\) en fonction de la dimension. Pour ce faire, on calcule un grand nombre d’estimateurs de \\(\\widehat\\beta_1\\) pour différentes valeurs de \\(p\\).\n\n\n\n\n\nOn met en forme les résultats\n\n\n\n\n\nPuis on compare, pour chaque dimension considérée, les distributions de \\(\\widehat\\beta_1\\) :\n\nen étudiant le biais et la variance\n\n\n\nen visualisant la distribution avec un boxplot\n\n\n\nOn retrouve bien que la dimension impacte notamment la variance des estimateurs."
  },
  {
    "objectID": "01-intro-grande-dim.html#exercices",
    "href": "01-intro-grande-dim.html#exercices",
    "title": "1  Les problèmes de la grande dimension",
    "section": "1.3 Exercices",
    "text": "1.3 Exercices\n\nExercice 1.1 (Distances entre deux points) Cet exercice est fortement inspiré de Giraud (2015). Soit \\(X^{(1)}=(X_1^{(1)},\\dots,X_p^{(1)})\\) et \\(X^{(2)}=(X_1^{(2)},\\dots,X_p^{(2)})\\) deux variables aléatoires indépendantes de loi uniforme sur l’hypercube \\([0,1]^p\\). Montrer que \\[\\mathbf E[\\|X^{(1)}-X^{(2)}\\|^2]=\\frac{p}{6}\\quad\\text{et}\\quad\\sigma[\\|X^{(1)}-X^{(2)}\\|^2]\\approx 0.2\\sqrt{p}.\\]\n\nSoit \\(U\\) et \\(U^\\prime\\) deux variables aléatoires indépendantes de loi uniforme sur \\([0,1]\\). On a \\[\\mathbf E[\\|X^{(1)}-X^{(2)}\\|^2]=\\sum_{k=1}^p\\mathbf E\\left[\\left(X_k^{(1)}-X_k^{(2)}\\right)\\right]=p\\mathbf E[(U-U^\\prime)^2]=p(2\\mathbf E[U^2]-2\\mathbf E[U]^2)=\\frac{p}{6}\\] car \\(\\mathbf E[U^2]=1/3\\) et \\(\\mathbf E[U]=1/2\\). De même \\[\\sigma[\\|X^{(1)}-X^{(2)}\\|^2]=\\sqrt{\\sum_{k=1}^p\\mathbf V\\left[\\left(X_k^{(1)}-X_k^{(2)}\\right)\\right]}=\\sqrt{p\\mathbf V[(U^\\prime-U)^2]}\\approx 0.2\\sqrt{p}\\] car \\[\\mathbf E\\left[(U^\\prime-U)^4\\right]=\\int_0^1\\int_0^1(x-y)^4\\,\\mathrm{d}x\\mathrm{d}y=\\frac{1}{15}\\] et donc \\(\\mathbf V[(U^\\prime-U)^2]=1/15-1/36\\approx 0.04\\).\n\n\n\nExercice 1.2 (Vitesse de convergence pour l’estimateur à noyau) On considère le modèle de régression \\[Y_i=m(x_i)+\\varepsilon_i,\\quad i=1,\\dots,n\\] où \\(x_1,\\dots,x_n\\in\\mathbb R^d\\) sont déterministes et \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) sont des variables i.i.d. d’espérance nulle et de variance \\(\\sigma^2<+\\infty\\). On désigne par \\(\\|\\,.\\,\\|\\) la norme Euclidienne dans \\(\\mathbb R^d\\). On définit l’estimateur localement constant de \\(m\\) en \\(x\\in\\mathbb R^d\\) par : \\[\\hat m(x)=\\mathop{\\mathrm{argmin}}_{a\\in\\mathbb R}\\sum_{i=1}^n(Y_i-a)^2K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] où \\(h>0\\) et pour \\(u\\in\\mathbb R,K(u)=\\mathbf 1_{[0,1]}(u)\\). On suppose que \\(\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)>0\\).\n\nDonner la forme explicite de \\(\\hat m(x)\\).\n\nEn annulant la dérivée par rapport à \\(a\\), on obtient \\[\\hat m(x)=\\frac{\\sum_{i=1}^nY_iK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\n\nMontrer que \\[\\mathbf V[\\hat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}\\] et \\[\\mathbf E[\\hat m(x)]-m(x)=\\frac{\\sum_{i=1}^n(m(x_i)-m(x))K\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\]\n\nCes propriétés se déduisent directement en remarquant que \\(\\mathbf V[Y_i]=\\sigma^2\\) et \\(\\mathbf E[Y_i]=m(x_i)\\).\n\nOn suppose maintenant que \\(m\\) est Lipschitzienne de constante \\(L\\), c’est-à-dire que \\(\\forall (x_1,x_2)\\in\\mathbb R^d\\times\\mathbb R^d\\) \\[|m(x_1)-m(x_2)|\\leq L\\|x_1-x_2\\|.\\] Montrer que \\[|\\textrm{biais}[\\hat m(x)]|\\leq Lh.\\]\n\nOn a \\(|m(x_i)-m(x)|\\leq L\\|x_i-x\\|\\). Or \\[K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] est non nul si et seulement si \\(\\|x_i-x\\|\\leq h\\). Donc pour tout \\(i=1,\\dots,n\\) \\[L\\|x_i-x\\|K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\leq Lh K\\left(\\frac{\\|x_i-x\\|}{h}\\right).\\] D’où le résultat.\n\nOn suppose de plus qu’il existe une constante \\(C_1\\) telle que \\[C_1\\leq\\frac{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}{n\\textrm{Vol}(B_h)},\\] où \\(B_h=\\{u\\in\\mathbb R^d:\\|u\\|\\leq h\\}\\) est la boule de rayon \\(h\\) dans \\(\\mathbb R^d\\) et \\(\\textrm{Vol}(A)\\) désigne le volume d’un ensemble \\(A\\subset\\mathbb R^d\\). Montrer que \\[\\mathbf V[\\hat m(x)]\\leq\\frac{C_2\\sigma^2}{nh^d},\\] où \\(C_2\\) est une constante dépendant de \\(C_1\\) et \\(d\\) à préciser.\n\nOn a \\[\\mathbf V[\\hat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}=\\frac{\\sigma^2}{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}.\\] Or \\[\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)\\geq C_1n\\textrm{Vol}(B_h)\\geq C_1\\gamma_dnh^d\\] où \\(\\gamma_d\\) désigne le volume de la boule unité en dimension \\(d\\). On a donc \\[\\mathbf V[\\hat m(x)]\\leq \\frac{\\sigma^2}{C_1\\gamma_dnh^d}.\\]\n\nDéduire des questions précédentes un majorant de l’erreur quadratique moyenne de \\(\\hat m(x)\\).\n\nOn déduit \\[\\mathbf E[(\\hat m(x)-m(x))^2]\\leq L^2h^2+\\frac{C_2\\sigma^2}{nh^d}.\\]\n\nCalculer \\(h_{\\text{opt}}\\) la valeur de \\(h\\) qui minimise ce majorant. Que vaut ce majorant lorsque \\(h=h_{\\text{opt}}\\) ? Comment varie cette vitesse lorsque \\(d\\) augmente ? Interpréter.\n\nSoit \\(M(h)\\) le majorant. On a \\[M(h)^\\prime=2hL^2-\\frac{C_2\\sigma^2d}{n}h^{-d-1}.\\] La dérivée s’annule pour \\[h_{\\text{opt}}=\\frac{2L^2}{C_2\\sigma^2d}n^{-\\frac{1}{d+2}}.\\] Lorsque \\(h=h_{\\text{opt}}\\) l’erreur quadratique vérifie \\[\\mathbf E[(\\hat m(x)-m(x))^2]=\\mathrm{O}\\left(n^{-\\frac{2}{d+2}}\\right).\\]\n\n\n\n\n\n\n\nGiraud, C. 2015. Introduction to High-Dimensional Statistics. CRC Press."
  },
  {
    "objectID": "02-reg-comp.html",
    "href": "02-reg-comp.html",
    "title": "2  Régression sur composantes",
    "section": "",
    "text": "Les performances des estimateurs classiques (MCO) des paramètres du modèle linéaire\n\\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_dX_d+\\varepsilon\\] peuvent se dégrader lorsque la dimension \\(d\\) est grande ou en présence de dépendance linéaire entre les variables explicatives. Les régressions sur composantes consistent à trouver de nouvelles composantes \\(Z_k,j=k,\\dots,q\\) avec \\(q\\leq p\\) qui s’écrivent le plus souvent comme des combinaisons linéaires des \\(X_j\\) dans l’idée de diminuer le nombre de paramètres du modèle ou la dépendance entre les covariables. Il existe plusieurs façons de construire ces composantes, dans cette partie nous proposons :\nNous commençons par un bref rappel sur la sélection de variables."
  },
  {
    "objectID": "02-reg-comp.html#sélection-de-variables",
    "href": "02-reg-comp.html#sélection-de-variables",
    "title": "2  Régression sur composantes",
    "section": "2.1 Sélection de variables",
    "text": "2.1 Sélection de variables\nOn considère le jeu de données ozone.txt où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable maxO3) par d’autres variables essentiellement météorologiques.\n\nozone <- read.table(\"data/ozone.txt\")\nhead(ozone)\n\n         maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v\n20010601    87 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946     84\n20010602    82 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87\n20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82\n20010604   114 16.2 19.7 22.5   1    1    0  0.9848  0.3473 -0.1736     92\n20010605    94 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301    114\n20010606    80 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000     94\n          vent pluie\n20010601  Nord   Sec\n20010602  Nord   Sec\n20010603   Est   Sec\n20010604  Nord   Sec\n20010605 Ouest   Sec\n20010606 Ouest Pluie\n\n\n\nAjuster un modèle linéaire avec lm et analyser la pertinence des variables explicatives dans le modèle.\n\n\n\n\nIl semble que quelques variables ne sont pas nécessaires dans le modèle.\n\nExpliquer les sorties de la commande\n\nlibrary(leaps)\nmod.sel <- regsubsets(maxO3~.,data=ozone,nvmax=14)\nsummary(mod.sel)\n\nSubset selection object\nCall: regsubsets.formula(maxO3 ~ ., data = ozone, nvmax = 14)\n14 Variables  (and intercept)\n          Forced in Forced out\nT9            FALSE      FALSE\nT12           FALSE      FALSE\nT15           FALSE      FALSE\nNe9           FALSE      FALSE\nNe12          FALSE      FALSE\nNe15          FALSE      FALSE\nVx9           FALSE      FALSE\nVx12          FALSE      FALSE\nVx15          FALSE      FALSE\nmaxO3v        FALSE      FALSE\nventNord      FALSE      FALSE\nventOuest     FALSE      FALSE\nventSud       FALSE      FALSE\npluieSec      FALSE      FALSE\n1 subsets of each size up to 14\nSelection Algorithm: exhaustive\n          T9  T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 Vx15 maxO3v ventNord ventOuest\n1  ( 1 )  \" \" \"*\" \" \" \" \" \" \"  \" \"  \" \" \" \"  \" \"  \" \"    \" \"      \" \"      \n2  ( 1 )  \" \" \"*\" \" \" \" \" \" \"  \" \"  \" \" \" \"  \" \"  \"*\"    \" \"      \" \"      \n3  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \" \" \" \"  \" \"  \"*\"    \" \"      \" \"      \n4  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \" \"  \"*\"    \" \"      \" \"      \n5  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \" \"  \"*\"    \" \"      \" \"      \n6  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \" \"      \n7  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \"*\"      \" \"      \n8  ( 1 )  \" \" \"*\" \" \" \"*\" \" \"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \"*\"      \n9  ( 1 )  \" \" \"*\" \" \" \"*\" \"*\"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \"*\"      \n10  ( 1 ) \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \" \"  \"*\"  \"*\"    \" \"      \"*\"      \n11  ( 1 ) \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \"*\"  \"*\"  \"*\"    \" \"      \"*\"      \n12  ( 1 ) \" \" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \"*\"  \"*\"  \"*\"    \"*\"      \"*\"      \n13  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\"  \" \"  \"*\" \"*\"  \"*\"  \"*\"    \"*\"      \"*\"      \n14  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\"  \"*\"  \"*\" \"*\"  \"*\"  \"*\"    \"*\"      \"*\"      \n          ventSud pluieSec\n1  ( 1 )  \" \"     \" \"     \n2  ( 1 )  \" \"     \" \"     \n3  ( 1 )  \" \"     \" \"     \n4  ( 1 )  \" \"     \" \"     \n5  ( 1 )  \" \"     \"*\"     \n6  ( 1 )  \" \"     \"*\"     \n7  ( 1 )  \" \"     \"*\"     \n8  ( 1 )  \"*\"     \"*\"     \n9  ( 1 )  \"*\"     \"*\"     \n10  ( 1 ) \"*\"     \"*\"     \n11  ( 1 ) \"*\"     \"*\"     \n12  ( 1 ) \"*\"     \"*\"     \n13  ( 1 ) \"*\"     \"*\"     \n14  ( 1 ) \"*\"     \"*\"     \n\n\n\nOn obtient une table avec des étoiles qui permettent de visualiser les meilleurs modèles à \\(1,2,\\dots,8\\) variables au sens du \\(R^2\\).\n\nSélectionner le meilleur modèle au sens du \\(R^2\\). Que remarquez-vous ?\n\n\n\n\nLe meilleur modèle est le modèle complet. C’est logique puisque le \\(R^2\\) va toujours privilégier le modèle le plus complexe, c’est un critère d'ajustement.\n\nFaire de même pour le \\(C_p\\) et le \\(BIC\\). Que remarquez-vous pour les variables explicatives qualitatives ?\n\n\n\n\nCes critères choisissent ici le même modèle, avec 4 variables. On remarque que les variables qualitatives ne sont pas réellement traitées comme des variables : une modalité est égale à une variable. Par conséquent, cette procédure ne permet pas vraiment de sélectionner des variables qualitatives.\n\nComparer cette méthode avec des modèles sélectionnées par la fonction step ou la fonction bestglm du package bestglm.\n\n\n\nLa fonction step permet de faire de la sélection pas à pas. Par exemple, pour une procédure descendante avec le critère \\(AIC\\) on utilisera :\nmod.step <- step(lin.complet,direction=\"backward\",trace=0)\nmod.step\nLa fonction bestglm permet quant à elle de faire des sélections exhaustive ou pas à pas, on peut l’utiliser pour tous les glm. Attention les variables qualitatives doivent être des facteurs et la variable à expliquer doit être positionnée en dernière colonne pour cette fonction."
  },
  {
    "objectID": "02-reg-comp.html#régression-sur-composantes-principales-méthodo",
    "href": "02-reg-comp.html#régression-sur-composantes-principales-méthodo",
    "title": "2  Régression sur composantes",
    "section": "2.2 Régression sur composantes principales (méthodo)",
    "text": "2.2 Régression sur composantes principales (méthodo)\nL’algorithme PCP est une méthode de réduction de dimension, elle consiste à faire un modèle linéaire MCO sur les premiers axes de l’ACP. On désigne par\n\n\\(\\mathbb X\\) la matrice qui contient les valeurs des variables explicatives que l’on suppose centrée réduite.\n\\(Z_1,\\dots,Z_p\\) les axes de l’ACP qui s’écrivent comme des combinaisons linéaires des variables explicatives : \\(Z_j=w_j^t X\\).\n\nL’algorithme PCR consiste à choisir un nombre de composantes \\(m\\) et à faire une régression MCO sur les \\(m\\) premiers axes de l’ACP : \\[Y=\\alpha_0+\\alpha_1 Z_1+\\dots+\\alpha_mZ_m+\\varepsilon.\\]\nSi on désigne par\n\n\\(x\\in\\mathbb R^d\\) une nouvelle observation que l’on a centrée réduite également;\n\\(z_1,\\dots,z_M\\) les coordonnées de \\(x\\) dans la base définie par les \\(m\\) premiers axes de l’ACP (\\(z_j=w_j^tx\\))\n\nl’algorithme PCR reverra la prévision \\[\\widehat m_{\\text{PCR}}(x)=\\widehat \\alpha_0+\\widehat \\alpha_1 z_1+\\dots+\\widehat \\alpha_mz_m.\\] Cette prévision peut s’écrire également comme une combinaison linéaire des variables explicatives (centrées réduites ou non) : \\[\\widehat m_{\\text{PCR}}(x)=\\widehat \\gamma_0+\\widehat \\gamma_1 \\tilde x_1+\\dots+\\widehat \\gamma_p \\tilde x_p=\\widehat \\beta_0+\\widehat \\beta_1 x_1+\\dots+\\widehat \\beta_p x_p,\\] \\(\\tilde x_j\\) désignant l’observation brute (non centrée réduite).\nL’exercice suivant revient sur cet algorithme et notamment sur le lien entre ces différents paramètres.\n\nExercice 2.1 (Régression PCR avec R) On considère le jeu de données Hitters dans lequel on souhaite expliquer la variable Salary par les autres variables du jeu de données. Pour simplifier le problème, on supprime les individus qui possèdent des données manquantes (il ne faut pas faire ça normalement !).\n\nlibrary(ISLR)\nHitters <- na.omit(Hitters)\n\n\nParmi les variables explicatives, certaines sont qualitatives. Expliquer comment, à l’aide de la fonction model.matrix on peut utiliser ces variables dans un modèle linéaire. On appellera X la matrice des variables explicatives construites avec cette variable.\n\nComme pour le modèle linéaire, on utilise des contraintes identifiantes. Cela revient à prendre une modalité de référence et à coder les autres modalités par 0-1.\n\n\n\n\nCalculer la matrice Xcr qui correspond à la matrice X centrée réduite. On pourra utiliser la fonction scale.\n\n\n\nA l’aide de la fonction PCA du package FactoMineR, effectuer l’ACP du tableau Xcr avec l’option scale.unit=FALSE.\n\nOn utilise ici scale.unit=FALSE car les données sont déjà centrées-réduites. Ça nous permet de contrôler cette étape.\n\n\n\n\nRécupérer les coordonnées des individus sur les 5 premiers axes de l’ACP (variables \\(Z\\) dans le cours).\n\n\n\nEffectuer la régression linéaire sur les 5 premières composantes principales et calculer les estimateurs des MCO (\\(\\widehat\\alpha_k,k=1,\\dots,5\\) dans le cours).\n\n\n\n\nRemarque :\n\nOn obtient ici les estimateurs des \\(\\alpha,j=1,\\dots,5\\).\n\non peut aussi tout faire “à la main” (sans utiliser PCA)\n\n\n\n\n\nEn déduire les estimateurs dans l’espace des données initiales pour les données centrées réduites, puis pour les données brutes. On pourra récupérer les vecteurs propres de l’ACP (\\(w_k\\) dans le cours) dans la sortie svd de la fonction PCA.\n\n\nPour les données centrées-réduites, les coefficients s’obtiennent avec les formules vues en cours\n\n\\[\\widehat\\beta_0=\\bar{\\mathbb Y}\\quad\\text{et}\\quad \\widehat\\beta_j=\\sum_{k=1}^m\\widehat\\alpha_kw_{kj}.\\]\n\n\n\nPour les données brutes, on utilise les formules :\n\\[\\widehat\\gamma_0=\\widehat\\beta_0-\\sum_{j=1}^p\\widehat\\beta_j\\mu_j\\quad\\text{et}\\quad\\widehat\\gamma_j=\\frac{\\widehat\\beta_j}{\\sigma_j}.\\]\n\n\n\n\n\nRetrouver les estimateurs dans l’espace des données initiales pour les données centrées réduites à l’aide de la fonction pcr du package pls.\n\n\n\n\nOn remarque que la fonction PCR renvoie les coefficients par rapport aux variables initiales centrées réduites. Cela fait du sens car il est dans ce cas possible de comparer les valeurs des estimateurs pour tenter d’interpréter le modèle. C’est beaucoup plus difficile à faire avec les coefficients des axes de l’ACP ou des variables intiales. Il est également important de noter que, contrairement aux estimateurs MCO du modèle linéaire Gaussien, on n’a pas d’information précise sur la loi des estimateurs, il n’est donc pas possible (ou pas facile) de faire des tests ou de calculer des intervalles de confiance.\n\nOn considère les individus suivants\n\ndf.new <- Hitters[c(1,100,80),]\n\nCalculer de 3 façons différentes les valeurs de salaire prédites par la régression sur 5 composantes principales.\n\n\nApproche classique : on utilise predict.pcr :\n\n\n\nOn considère les valeurs centrées réduites et on utilise : \\[\\widehat m_{\\text{PCR}}(x)=\\widehat\\beta_0+\\widehat\\beta_1x_1+\\dots+\\widehat\\beta_px_p.\\]\n\n\n\nOn considère les données brutes et on utilise : \\[\\widehat m_{\\text{PCR}}(x)=\\widehat\\gamma+\\widehat\\gamma_1\\tilde x_1+\\dots+\\widehat\\gamma_p\\tilde x_p.\\]\n\n\n\n\n\n\n\n\n\n\n\nExercice 2.2 (Composantes PCR) On rappelle que les poids \\(w_k\\) des composantes principales s’obtiennent en résolvant le problème :\n\\[\\max_{w\\in\\mathbb R^d}\\mathbf V(\\mathbb Xw)\\] \\[\\text{sous les contraintes }\\|w\\|=1,w^t\\mathbb X^t\\mathbb X w_\\ell=0, \\ell=1,\\dots,k-1.\\]\n\nMontrer \\(w_1\\) est un vecteur propre associé à la plus grande valeur propre de \\(\\mathbb X^t\\mathbb X\\).\n\nOn écrit le Lagrangien \\[L(w,\\lambda)=w^t\\mathbb X^t\\mathbb Xw-\\lambda(w^tw-1).\\] et on le dérive par rapport à \\(w\\) : \\[\\frac{\\partial L}{\\partial w}(w,\\lambda)=2\\mathbb X^t\\mathbb Xw-2\\lambda w.\\] En annulant cette dérivée, on déduit que \\(w_1\\) est un vecteur propre de \\(\\mathbb X^t\\mathbb X\\). De plus, si \\(w\\) est vecteur propre unitaire de \\(\\mathbb X^t\\mathbb X\\) associé à la valeur propre \\(\\lambda\\) on a \\(\\mathbf V(\\mathbb Xw)=\\lambda\\). On déduit que \\(w_1\\) est un vecteur propre associé à la plus grande valeur propre de \\(\\mathbb X^t\\mathbb X\\).\n\nCalculer \\(w_2\\).\n\nOn écrit le Lagrangien \\[L(w,\\lambda,\\mu)=w^t\\mathbb X^t\\mathbb Xw-\\lambda(w^tw-1)-\\mu w^t\\mathbb X^t\\mathbb Xw_1\\] et on calcule les dérivées partielles : \\[\\frac{\\partial L}{\\partial w}(w,\\lambda,\\mu)=2\\mathbb X^t\\mathbb Xw-2\\lambda w-\\mu\\mathbb X^t\\mathbb Xw_1.\\] \\[\\frac{\\partial L}{\\partial \\lambda}(w,\\lambda,\\mu)=w^tw-1\\quad\\text{et}\\quad\\frac{\\partial L}{\\partial \\mu}(w,\\lambda,\\mu)=-w^t\\mathbb X^t\\mathbb Xw_1.\\] En multipliant la première dérivée partielle par \\(w_1^t\\) et en utilisant le fait que \\(W_1\\) est un vecteur propre de \\(\\mathbb X^t\\mathbb X\\), on déduit que \\(\\mu=0\\). Par conséquent, \\(w_2\\) est un vecteur propre associé à la deuxième plus grande valeur propre de \\(\\mathbb X^t\\mathbb X\\)."
  },
  {
    "objectID": "02-reg-comp.html#régression-pls-méthodo",
    "href": "02-reg-comp.html#régression-pls-méthodo",
    "title": "2  Régression sur composantes",
    "section": "2.3 Régression PLS : méthodo",
    "text": "2.3 Régression PLS : méthodo\nLa régression PLS propose de construire également de nouvelles composantes comme des combinaisons linéaires des variables explicatives. Comme pour l’algorithme PCR, les composantes sont calculées les unes après les autres et orthogonales entre elles. La principale différence et qu’on ne cherche pas les composantes qui maximisent la variabilités des observations projetées, mais les composantes qui maximisent la colinéarité avec la cible. L’algorithme est expliqué dans l’exercice suivant.\n\nExercice 2.3 (Calcul des composantes PLS) On reprend les notations du cours : \\(\\mathbb Y\\) désigne le vecteur de la variable à expliquer et \\(\\mathbb X\\) la matrice qui contient les observations des variables explicatives. On la suppose toujours centrée réduite.\n\nOn pose \\(\\mathbb Y^{(1)}=\\mathbb Y\\) et \\(\\mathbb X^{(1)}=\\mathbb X\\). On cherche \\(Z_1=w_1^tX^{(1)}\\) qui maximise \\[\\langle \\mathbb X^{(1)}w_1,\\mathbb Y^{(1)}\\rangle\\quad\\text{sous la contrainte}\\quad\\|w\\|^2=1.\\] Cela revient à cherche la combinaison linéaire des colonnes de \\(\\mathbb X^{(1)}\\) la plus corrélée à \\(\\mathbb Y^{(1)}\\). Calculer cette première composante.\n\nOn écrit le lagrangien \\[L(x,\\lambda)={\\mathbb Y^{(1)}}^t\\mathbb X^{(1)}w_1-\\frac{1}{2}\\lambda(\\|w_1\\|^2-1)\\] En dérivant par rapport à \\(w\\) et \\(\\lambda\\) on obtient les équations \\[\\left\\{\n\\begin{array}{l}\n{\\mathbb X^{(1)}}^t\\mathbb Y^{(1)}-\\lambda w_1=0 \\\\\n\\|w_1\\|^2=1\n\\end{array}\\right.\\] La solution est donnée par \\[w_1=\\frac{{\\mathbb X^{(1)}}^t\\mathbb Y^{(1)}}{{\\|\\mathbb X^{(1)}}^t\\mathbb Y^{(1)}\\|}.\\]\n\nOn pose \\(Z_1=w_1^tX^{(1)}\\) et \\(\\mathbb Z_1=\\mathbb X^{(1)}w_1\\). On considère le modèle de régression linéaire \\[Y^{(1)}=\\alpha_0+\\alpha_1Z_1+\\varepsilon.\\] Exprimer les estimateurs MCO de \\(\\alpha=(\\alpha_0,\\alpha_1)\\) en fonction de \\(\\mathbb Z^{(1)}\\) et \\(\\mathbb Y^{(1)}\\).\n\nOn déduit \\[\\widehat \\alpha_0=\\bar{\\mathbb Y}^{(1)}-\\widehat \\alpha_1\\bar{\\mathbb Z}_1=\\bar{\\mathbb Y}^{(1)}\\] car \\(\\bar{\\mathbb Z}_1=0\\) puisque \\(\\mathbb X^{(1)}\\) est centrée. Le second estimateur s’obtient par \\[\\widehat \\alpha_1=\\frac{\\langle \\mathbb Z_1,\\mathbb Y^{(1)}\\rangle}{\\langle \\mathbb Z_1,\\mathbb Z_1\\rangle}.\\]\n\nOn passe maintenant à la deuxième composante. On cherche à expliquer la partie résiduelle \\[\\mathbb Y^{(2)}=P_{Z_1^\\perp}(\\mathbb Y^{(1)})=\\widehat\\varepsilon_1=\\mathbb Y^{(1)}-\\widehat{\\mathbb Y}^{(1)}\\] par la “meilleure” combinaison linéaire orthogonale à \\(Z_1\\). On orthogonalise chaque \\(\\tilde{\\mathbb X}_j^{(1)}\\) par rapport à \\(\\mathbb Z_1\\) : \\[{\\mathbb X}_j^{(2)}=P_{\\mathbb Z_1^\\perp}({\\mathbb X}_j^{(1)})=(\\text{Id}-P_{\\mathbb Z_1})({\\mathbb X}_j^{(1)})={\\mathbb X}_j^{(1)}-\\frac{\\langle \\mathbb Z_1,{\\mathbb X}_j^{(1)}\\rangle}{\\langle \\mathbb Z_1,\\mathbb Z_1\\rangle}\\mathbb Z_1.\\] et on déduit \\(w_2\\) comme \\(w_1\\) : \\(w_2=\\tilde{\\mathbb X}^{(2)'}\\mathbb Y^{(2)}\\). On considère ensuite le modèle \\(Y^{(2)}=\\alpha_2Z_2+\\varepsilon\\). Exprimer l’estimateur des MCO de \\(\\alpha_2\\) en fonction de \\(\\mathbb Z_2=\\mathbb X^{(2)}w_2\\) et \\(\\mathbb Y\\).\n\nOn a \\[\\widehat\\alpha_2=\\frac{\\langle \\mathbb Z_2,\\mathbb Y^{(2)}\\rangle}{\\langle \\mathbb Z_2,\\mathbb Z_2\\rangle}=\\frac{\\langle \\mathbb Z_2,\\mathbb Y-\\widehat{\\mathbb Y}^{(1)}\\rangle}{\\langle \\mathbb Z_2,\\mathbb Z_2\\rangle}=\\frac{\\langle \\mathbb Z_2,\\mathbb Y\\rangle}{\\langle \\mathbb Z_2,\\mathbb Z_2\\rangle}\\] car \\(\\widehat{\\mathbb Y}^{(1)}=\\widehat \\alpha_0+\\widehat \\alpha_1\\mathbb Z_1\\) est orthogonal à \\(\\mathbb Z_2\\).\n\n\n\n\nExercice 2.4 (Régression PLS sur R) On considère les mêmes données que précédemment.\n\nA l’aide du vecteur \\(\\mathbb Y\\) (Salary) et de la matrice des \\(\\mathbb X\\) centrées réduites calculées dans l’Exercice 2.1, calculer la première composante PLS \\(\\mathbb Z_1\\).\n\n\n\nEn déduire le coefficient associé à cette première composante en considérant le modèle \\[Y=\\alpha_1 Z_1+\\varepsilon.\\]\n\n\n\nEn déduire les coefficients en fonction des variables initiales (centrées réduites) de la régression PLS à une composante \\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p+\\varepsilon.\\]\n\n\n\nRetrouver ces coefficients en utilisant la fonction plsr."
  },
  {
    "objectID": "02-reg-comp.html#comparaison-pcr-vs-pls.",
    "href": "02-reg-comp.html#comparaison-pcr-vs-pls.",
    "title": "2  Régression sur composantes",
    "section": "2.4 Comparaison : PCR vs PLS.",
    "text": "2.4 Comparaison : PCR vs PLS.\n\nSéparer le jeu de données (Hitters toujours) en un échantillon d’apprentissage de taille 200 et un échantillon test de taille 63.\n\n\n\nAvec les données d’apprentissage uniquement construire les régressions PCR et PLS. On choisira les nombres de composantes par validation croisée.\n\n\n\n\n\n\nComparer les deux méthodes en utilisant l’échantillon de validation. On pourra également utiliser un modèle linéaire classique.\n\n\n\n\n\n\n\n\n\nComparer ces méthodes à l’aide d’une validation croisée 10 blocs.\n\nAttention il ne s’agit pas ici de sélectionner les nombres de composantes par validation croisée. On veut comparer :\n\nl’algorithme **PCR** qui sélectionne le nombre de composantes par validation croisée à\nl’algorithme **PLS** qui sélectionne le nombre de composantes par validation croisée.\n\nOn définit d’abord les 10 blocs pour la validation croisée :\n\n\n\n\n\nPuis on fait la validation croisée (en sélectionnant le nombre de composantes par validation croisée) à chaque étape :\n\n\n\n\n\n\n\n\nOn compare à un modèle qui prédit toujours la moyenne :\n\n\n\n\n\nOn peut retenter l’analyse en considérant toutes les interactions d’ordre 2 :\n\n\n\n\n\nOn obtient les performances suivantes :\n\n\n\n\n\nOn mesure bien l’intérêt de réduire la dimension dans ce nouveau contexte."
  },
  {
    "objectID": "03-ridge-lasso.html",
    "href": "03-ridge-lasso.html",
    "title": "3  Régressions pénalisées (ou sous contraintes)",
    "section": "",
    "text": "Nous considérons toujours le modèle linéaire\n\\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_dX_d+\\varepsilon\\] Lorsque \\(d\\) est grand ou que les variables sont linéairement dépendantes, les estimateurs des moindres carrées peuvent être mis en défaut. Les méthodes pénalisées ou sous contraintes consistent alors à restreindre l’espace sur lequel on minimise ce critère. On va alors chercher le vecteur \\(\\beta\\) qui minimise\n\\[\\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^dx_{ij}\\beta_j\\right)^2\\quad\\text{sous la contrainte }\\quad\\sum_{j=1}^d\\beta_j^2\\leq t\\] ou de façon équivalente (dans le sens où il existe une équivalence entre \\(t\\) et \\(\\lambda\\))\n\\[\\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^dx_{ij}\\beta_j\\right)^2+\\lambda\\sum_{j=1}^d\\beta_j^2.\\] Les estimateurs obtenus sont les estimateurs ridge. Les estimateurs lasso s’obtiennent en remplaçant la contrainte ou la pénalité par une norme 1 (\\(\\sum_{j=1}^d|\\beta_j|\\)). Nous présentons dans cette partie les étapes principales qui permettent de faire ce type de régression avec R. Le package le plus souvent utilisé est glmnet."
  },
  {
    "objectID": "03-ridge-lasso.html#ridge-et-lasso-avec-glmnet",
    "href": "03-ridge-lasso.html#ridge-et-lasso-avec-glmnet",
    "title": "3  Régressions pénalisées (ou sous contraintes)",
    "section": "3.1 Ridge et lasso avec glmnet",
    "text": "3.1 Ridge et lasso avec glmnet\nOn considère le jeu de données ozone.txt où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable maxO3) par d’autres variables essentiellement météorologiques.\n\nozone <- read.table(\"data/ozone.txt\")\nhead(ozone)\n\n         maxO3   T9  T12  T15 Ne9 Ne12 Ne15     Vx9    Vx12    Vx15 maxO3v\n20010601    87 15.6 18.5 18.4   4    4    8  0.6946 -1.7101 -0.6946     84\n20010602    82 17.0 18.4 17.7   5    5    7 -4.3301 -4.0000 -3.0000     87\n20010603    92 15.3 17.6 19.5   2    5    4  2.9544  1.8794  0.5209     82\n20010604   114 16.2 19.7 22.5   1    1    0  0.9848  0.3473 -0.1736     92\n20010605    94 17.4 20.5 20.4   8    8    7 -0.5000 -2.9544 -4.3301    114\n20010606    80 17.7 19.8 18.3   6    6    7 -5.6382 -5.0000 -6.0000     94\n          vent pluie\n20010601  Nord   Sec\n20010602  Nord   Sec\n20010603   Est   Sec\n20010604  Nord   Sec\n20010605 Ouest   Sec\n20010606 Ouest Pluie\n\n\nContrairement à la plupart des autres package R qui permettent de faire de l’apprentissage, le package glmnet n’autorise pas l’utilisation de formules : il faut spécifier explicitement la matrice des \\(X\\) et le vecteur des \\(Y\\). On peut obtenir la matrice des \\(X\\) et notamment le codage des variables qualitatives avec la fonction model.matrix:\n\nozone.X <- model.matrix(maxO3~.,data=ozone)[,-1]\nozone.Y <- ozone$maxO3\n\n\nCharger le package glmnet et à l’aide de la fonction glmnet calculer les estimateurs ridge et lasso.\n\n\n\nAnalyser les sorties qui se trouvent dans les arguments lambda et beta de glmnet.\n\nLa fonction glmnet calcule tous les estimateurs pour une grille de valeurs de lambda spécifiée ici :\n\n\n\n\n\nOn peut récupérer les valeurs de beta associées à chaque valeur de la grille avec\n\n\n\n\nVisualiser les chemins de régularisation des estimateurs ridge et lasso. On pourra utiliser la fonction plot.\n\n\n\nSélectionner les paramètres de régularisation à l’aide de la fonction cv.glmnet. On pourra notamment faire un plot de l’objet et expliquer le graphe obtenu.\n\nCommençons par ridge :\n\n\n\n\n\nOn visualise les erreurs quadratiques calculées par validation croisée 10 blocs en fonction de lambda (échelle logarithmique). Deux traites verticaux sont représentés :\n\ncelui de gauche correspond à la valeur de `lambda` qui minimise l’erreur quadratique ;\ncelui de droite correspond à la plus grande valeur de `lambda` telle que l’erreur ne dépasse pas l’erreur minimale + 1 écart-type estimé de cette erreur.\n\nD’un point de vu pratique, cela signifie que l’utilisateur peut choisir n’importe quelle valeur de lambda entre les deux traits verticaux. Si on veut diminuer la complexité du modèle on choisira la valeur de droite. On peut obtenir ces deux valeurs avec\n\n\n\n\n\nOn peut faire de même pour le lasso :\n\n\n\n\nOn souhaite prédire la variable cible pour de nouveaux individus, par exemple les 25ème et 50ème individus du jeu de données. Calculer les valeurs prédites pour ces deux individus.\n\nUne première approche pourrait consister à réajuster le modèle sur toutes les données pour la valeur de lambda sélectionnée. Cette étape est en réalité déjà effectuée par la fonction cv.glmnet. Il suffit par conséquent d’appliquer la fonction predict à l’objet obtenu avec cv.glmnet en spécifiant la valeur de lambda souhaitée. Par exemple pour ridge :\n\n\n\n\n\nOn peut faire de même pour le lasso :\n\n\n\n\nA l’aide d’une validation croisée, comparer les performances des estimateurs MCO, ridge et lasso. On pourra utiliser les données ozone_complet.txt qui contiennent plus d’individus et de variables.\n\nozone1 <- read.table(\"data/ozone_complet.txt\",sep=\";\") %>% na.omit()\nozone1.X <- model.matrix(maxO3~.,data=ozone1)[,-1]\nozone1.Y <- ozone1$maxO3\n\n\nOn crée une fonction qui calcule les erreurs quadratiques par validations croisée des 3 procédures d’estimation.\n\n\n\n\n\n\n\n\nOn remarque que les approches régularisées n’apportent rien par rapport aux estimateurs MCO ici. Ceci peut s’expliquer par le fait que le nombre de variables n’est pas très important.\n\nRefaire la question précédente en considérant toutes les interactions d’ordre 2.\n\n\n\n\nLes méthodes régularisées permettent ici de diminuer les erreurs quadratiques de manière intéressante. Cela vient certainement du fait du nombre de variables explicatives qui est beaucoup plus important lorsqu’on prend en compte toutes les interactions d’ordre 2, nous en avons en effet 253 :"
  },
  {
    "objectID": "03-ridge-lasso.html#reconstruction-dun-signal",
    "href": "03-ridge-lasso.html#reconstruction-dun-signal",
    "title": "3  Régressions pénalisées (ou sous contraintes)",
    "section": "3.2 Reconstruction d’un signal",
    "text": "3.2 Reconstruction d’un signal\nLe fichier signal.csv contient un signal que l’on peut représenter par une fonction \\(m:\\mathbb R\\to\\mathbb R\\). On le visualise\n\nsignal <- read_csv(\"data/signal.csv\")\nggplot(signal)+aes(x=x,y=y)+geom_line()\n\n\n\n\nPlaçons nous dans le cas où on ne dispose que d’une version bruitée de ce signal. La courbe n’est pas observée mais on dispose d’un échantillon \\((x_i,y_i),i=1,\\dots,n\\) généré selon le modèle\n\\[y_i=m(x_i)+\\varepsilon_i.\\]\nLe fichier ech_signal.csv contient \\(n=60\\) observations issues de ce modèle. On représente les données et la courbe\n\ndonnees <- read_csv(\"data/ech_signal.csv\")\nggplot(signal)+aes(x=x,y=y)+geom_line()+\n  geom_point(data=donnees,aes(x=X,y=Y))\n\n\n\n\nNous cherchons dans cette partie à reconstruire le signal à partir de l’échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \\[y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\\] n’est pas approprié. De nombreuses approches en traitement du signal proposent d’utiliser une base ou dictionnaire représentée par une collection de fonctions \\(\\{\\psi_j(x)\\}_{j=1,\\dots,K}\\) et de décomposer le signal dans cette base :\n\\[m(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\\]\nPour un dictionnaire donné, on peut alors considérer un modèle linéaire\n\\[\\begin{equation}\n  y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x_i)+\\varepsilon_i.\n  (\\#eq:mod-lin-signal)\n\\end{equation}\\]\nLe problème est toujours d’estimer les paramètres \\(\\beta_j\\) mais les variables sont maintenant définies par les élements du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par\n\\[\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\\]\n\nÉcrire une fonction R qui admet en entrée :\n\nune grille de valeurs de x (un vecteur)\nune valeur de K (un entier positif)\n\net qui renvoie en sortie une matrice qui contiennent les valeurs du dictionnaire pour chaque valeur de x. Cette matrice devra donc contenir 2K colonnes et le nombre de lignes sera égal à la longueur du vecteur x.\n\n\n\nOn fixe K=25. Calculer les estimateurs des moindres carrés du modèle @ref(eq:mod-lin-signal).\n\nIl suffit d’ajuster le modèle linéaire où les variables explicatives sont données par le dictionnaire :\n\n\n\n\nReprésenter le signal estimé. Commenter le graphe.\n\n\n\n\nLe signal estimé a tendance à surajuster les données. Cela vient du fait que on estime 51 paramètres avec seulement 60 observations.\n\nCalculer les estimateurs lasso et représenter le signal issu de ces estimateurs.\n\nOn regarde tout d’abord le chemin de régularisation des estimateurs lasso\n\n\n\n\n\nIl semble que quelques coefficients quittent la valeur 0 bien avant les autres. On effectue maintenant la validation croisée pour sélectionner le paramètre \\(\\lambda\\).\n\n\n\n\n\nOn calcule les prévisions et on trace le signal.\n\n\n\n\n\nL’algorithme lasso a permis de corriger le problème de sur-apprentissage.\n\nIdentifier les coefficients lasso sélectionnés qui ne sont pas nuls.\n\n\n\nAjouter les signaux ajustés par les algorithme PCR et PLS.\n\n\nOn effectue la PCR :\n\n\n\n\n\n\nPuis la PLS :\n\n\n\n\n\n\nOn trace les signaux :\n\n\n\n\nOn peut également obtenir les erreurs quadratiques (puisqu’on connait la vraie courbe)"
  },
  {
    "objectID": "03-ridge-lasso.html#régression-logistique-pénalisée",
    "href": "03-ridge-lasso.html#régression-logistique-pénalisée",
    "title": "3  Régressions pénalisées (ou sous contraintes)",
    "section": "3.3 Régression logistique pénalisée",
    "text": "3.3 Régression logistique pénalisée\nOn considère le jeu de données sur la détection d’images publicitaires disponible ici https://archive.ics.uci.edu/ml/datasets/internet+advertisements.\n\nad.data <- read.table(\"data/ad_data.txt\",header=FALSE,sep=\",\",dec=\".\",\n                      na.strings = \"?\",strip.white = TRUE)\nnames(ad.data)[ncol(ad.data)] <- \"Y\"\nad.data$Y <- as.factor(ad.data$Y)\n\nLa variable à expliquer est\n\nsummary(ad.data$Y)\n\n   ad. nonad. \n   459   2820 \n\n\nCette variable est binaire. On considère une régression logistique pour expliquer cette variable. Le nombre de variables explicatives étant important, comparer les algorithmes du maximum de vraisemblance aux algorithmes de type ridge/lasso en faisant une validation croisée 10 blocs. On pourra utiliser comme critère de comparaison l’erreur de classification, la courbe ROC et l’AUC. Il faudra également prendre des décisions pertinentes vis-à-vis des données manquantes…\n\nOn commence par regarder les données manquantes :\n\n\n\n\n\nOn remarque que 920 individus ont au moins une donnée manquante alors que seules les 4 premières variables ont des données manquantes, on choisit donc de supprimer ces 4 variables.\n\n\n\n\n\nOn construit les matrices des variables explicatives pour les méthodes lasso et ridge (glmnet veut les variables explicatives sous forme de matrices).\n\n\n\n\n\nAvant de faire la validation croisée, nous présentons juste comment faire l’algorithme lasso. Comme pour la régression, on utilise la fonction cv.glmnet, il faut juste ajouter l’argument family=\"binomial\" :\n\n\n\n\n\nPar défaut le critère utilisé pour la classification binaire est celui de la déviance. On peut utiliser d’autres critères comme l’erreur de classification ou l’auc en modifiant l’argument type.measure. On gardera la déviance dans la suite. On peut maintenant faire la validation croisée 10 blocs pour calculer les prévisions des 3 algorithmes.\n\n\n\n\n\n\n\n\n\n\n\nLe tibble score contient, pour chaque individu, les prévisions des probabilités a posteriori \\[\\mathbf P(Y=\\text{nonad.}|X=x_i),\\quad i=1,\\dots,n.\\]\nOn peut déduire de ce tableau les critères souhaités :\n\nles courbes ROC:\n\n\n\n\n\n\nles AUC:\n\n\n\n\n\n\nles erreurs de classification :\n\n\n\n\nOn remarque que les méthodes pénalisées sont nettement meilleures que l’approche classique par maximum de vraisemblance sur cet exemple."
  },
  {
    "objectID": "03-ridge-lasso.html#exo-ridgelasso",
    "href": "03-ridge-lasso.html#exo-ridgelasso",
    "title": "3  Régressions pénalisées (ou sous contraintes)",
    "section": "3.4 Exercices",
    "text": "3.4 Exercices\n\nExercice 3.1 (Estimateurs ridge pour le modèle linéaire) On considère le modèle de régression \\[Y_i=\\beta_1x_{i1}+\\dots+\\beta_px_{ip}+\\varepsilon_i\\] où les \\(\\varepsilon_i\\) sont i.i.d de loi \\(\\mathcal N(0,\\sigma^2)\\). Pour \\(\\lambda\\geq 0\\), on note \\(\\hat\\beta_R(\\lambda)\\) l’estimateur ridge défini par \\[\n\\hat\\beta_R(\\lambda)=\\mathop{\\mathrm{argmin}}_\\beta\\sum_{i=1}^n\\left(y_i-\\sum_{j=1}^px_{ij}\\beta_j\\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2.\n\\]\n\nExprimer \\(\\hat\\beta_R(\\lambda)\\) en fonction de \\(\\mathbb X\\), \\(\\mathbb Y\\) et \\(\\lambda\\).\n\nLe critère à minimiser se réécrit \\[\\mathcal C(\\beta)=(\\mathbb Y-\\mathbb X\\beta)^t (\\mathbb Y-\\mathbb X\\beta)+\\lambda\\beta^t\\beta.\\] L’estimateur ridge est donc solution de \\[-2\\mathbb X^t\\mathbb Y+2\\mathbb X^t\\mathbb X\\beta+2\\lambda\\beta=0,\\] d’où \\[\\hat\\beta_R(\\lambda)=(\\mathbb X^t\\mathbb X+\\lambda I)^{-1}\\mathbb X^t\\mathbb Y.\\]\n\nÉtudier le biais et la variance de \\(\\hat\\beta_R(\\lambda)\\) en fonction de \\(\\lambda\\). On pourra également faire la comparaison avec l’estimateur des MCO.\n\nComme \\(\\mathbb Y=\\mathbb X\\beta+\\varepsilon\\), on obtient\n\\[\\begin{align*}\n\\esp[\\hat\\beta_R(\\lambda)]-\\beta & =(\\mathbb X^t\\mathbb X+\\lambda I)^{-1}\\mathbb X^t\\mathbb X\\beta-\\beta \\\\\n& =\\left[(\\mathbb X^t\\mathbb X+\\lambda I)^{-1}(\\mathbb X^t\\mathbb X-(\\mathbb X^t\\mathbb X+\\lambda I))\\right]\\beta \\\\\n& = -\\lambda(\\mathbb X^t\\mathbb X+\\lambda I)^{-1}\\beta.\n\\end{align*}\\]\nDe même, on obtient pour la variance \\[\n\\mathbf V(\\hat\\beta_R(\\lambda))=\\sigma^2(\\mathbb X^t\\mathbb X+\\lambda\\mathbb I)^{-1}\\mathbb X^t\\mathbb X(\\mathbb X^t\\mathbb X+\\lambda\\mathbb I)^{-1}.\n\\] La variance diminue lorsque \\(\\lambda\\) augmente, mais on remarque une augmentation du bais par rapport à l’estimateur des moindres carrés (et réciproquement lorsque \\(\\lambda\\) diminue).\n\nOn suppose que la matrice \\(\\mathbb X\\) est orthogonale. Exprimer les estimateurs \\(\\hat\\beta_{R,j}(\\lambda)\\) en fonction des estimateurs des MCO \\(\\hat\\beta_j, j=1,\\dots,p\\). Interpréter.\n\nSi \\(\\mathbb X\\) est orthogonale, alors \\[\\hat\\beta_R(\\lambda)=\\frac{1}{1+\\lambda}\\mathbb X^t\\mathbb Y=\\frac{\\hat\\beta_{MCO}}{1+\\lambda}.\\]\n\n\n\n\nExercice 3.2 (Estimateurs lasso dans le cas orthogonal) Cet exercice est inspiré de Giraud (2015). On rappelle qu’une fonction \\(F:\\mathbb R^n\\to\\mathbb R\\) est convexe si \\(\\forall x,y\\in\\mathbb R^n\\), \\(\\forall\\lambda\\in[0,1]\\) on a \\[F(\\lambda x+(1-\\lambda) y)\\leq \\lambda F(x)+(1-\\lambda)F(y).\\] On définit la sous-différentielle d’une fonction convexe \\(F\\) par \\[\\partial F(x)=\\{w\\in\\mathbb R^n:F(y)\\geq F(x)+\\langle w,y-x\\rangle\\textrm{ pour tout }y\\in\\mathbb R^n\\}.\\] On admettra que les minima d’une fonction convexe \\(F:\\mathbb R^n\\to\\mathbb R\\) sont caractérisés par \\[x^\\star\\in\\mathop{\\mathrm{argmin}}_{x\\in\\mathbb R^n}F(x)\\Longleftrightarrow 0\\in \\partial F(x^\\star)\\] et que \\(\\partial F(x)=\\{\\nabla F(x)\\}\\) lorsque \\(F\\) est différentiable en \\(x\\).\n\nMontrer que pour \\(x\\in\\mathbb R\\) \\[\n\\partial |x|=\\left\\{\n\\begin{array}{ll}\n\\textrm{signe}(x)   & \\textrm{si } x\\neq 0 \\\\\n\\left[-1;1\\right] & \\textrm{sinon,}\n\\end{array}\\right.\n\\] où \\(\\text{signe}(x)=\\mathbf 1_{x>0}-\\mathbf 1_{x\\leq 0}\\).\n\n\\(x\\mapsto|x|\\) est dérivable partout sauf en 0 donc \\(\\partial |x|=\\textrm{signe}(x)1\\) si \\(x\\neq 0\\). De plus, si \\(x=0\\) \\[\\partial|x|=\\{w\\in\\mathbb R:|y|\\geq \\langle w,y\\rangle\\ \\forall y\\in\\mathbb R\\}=\\{w\\in\\mathbb R:|y|\\geq wy\\ \\forall y\\in\\mathbb R\\}=[-1,1].\\]\n\nSoit \\(x\\in\\mathbb R^n\\).\n\nMontrer que \\[\\partial\\|x\\|_1=\\{w\\in\\mathbb R^n:\\langle w,x\\rangle=\\|x\\|_1\\text{ et }\\|w\\|_\\infty\\leq 1\\}.\\] On pourra utiliser que pour tout \\(p,q\\) tels que \\(1/p+1/q=1\\) on a \\[\\|x\\|_p=\\sup\\left\\{\\langle w,x\\rangle:\\|w\\|_q\\leq 1\\right\\}.\\]\n\nOn montre la double inclusion. Soit \\(w\\) tel que \\(\\langle w,x\\rangle=\\|x\\|_1\\) et \\(\\|w\\|_\\infty=1\\). On a \\(\\forall y\\in\\mathbb R^n\\), \\[\\|y\\|_1\\geq \\langle w,y\\rangle=\\langle w,y-x+x\\rangle=\\|x\\|_1+\\langle w,y-x\\rangle.\\] Donc \\(w\\in\\partial\\|x\\|_1\\). Inversement, soit \\(w\\in\\partial\\|x\\|_1\\). Par définition \\[\\partial\\|x\\|_1=\\{w\\in\\mathbb R^n:\\|y\\|_1\\geq \\langle w,y-x\\rangle+\\|x\\|_1\\ \\forall y\\in\\mathbb R^n\\}.\\] Pour \\(y=0\\) et \\(y=2x\\), on a donc \\[\\|x\\|_1\\leq\\langle w,x\\rangle\\quad\\textrm{et}\\quad 2\\|x\\|_1\\geq \\langle w,x\\rangle+\\|x\\|_1\\] d’où \\(\\|x\\|_1=\\langle x,w\\rangle=\\sum_iw_ix_i\\). De plus en posant \\(\\tilde w=(0,\\dots,0,\\text{signe}(w_i),0,\\dots,0)\\) où la coordonnée non nulle correspond au \\(\\max_i(|w_i|)\\) on a \\(\\|w\\|_\\infty=\\langle w,\\tilde w\\rangle\\) et \\(\\|\\tilde w\\|_\\infty=\\|\\tilde w\\|_1=1\\). De plus \\[\\|\\tilde w\\|_1\\geq \\|x\\|_1+\\langle w,\\tilde{w}-x\\rangle=\\|w\\|_\\infty\\quad\\Longrightarrow \\|w\\|_\\infty\\leq \\|\\tilde w\\|_1=1.\\]\n\nEn déduire \\[\\partial\\|x\\|_1=\\{w\\in\\mathbb R^n:w_j=\\textrm{signe}(x_j)\\textrm{ si }x_j\\neq 0, w_j\\in[-1,1]\\textrm{ si }x_j=0\\}.\\]\n\nOn a\n\\[\\begin{align*}\n\\partial\\|x\\|_1 & =\\{w\\in\\R^n:\\ps{w}{x}=\\|x\\|_1\\text{ et }\\|w\\|_\\infty\\leq 1\\}\\\\\n& = \\{w\\in\\R^n:\\sum_{i=1}^n(w_ix_i-|x_i|)=0\\text{ et }\\|w\\|_\\infty\\leq 1\\}.\n\\end{align*}\\]\nOr si \\(\\|w\\|_\\infty\\leq 1\\) alors \\(w_ix_i-|x_i|\\leq 0\\) \\(\\forall i=1,\\dots,n\\). Donc\n\\[\\begin{align*}\n\\partial\\|x\\|_1 & =\\{w\\in\\R^n:(w_ix_i-|x_i|)=0,i=1,\\dots,n\\text{ et }\\|w\\|_\\infty\\leq 1\\}\\\\\n&=\\{w\\in\\R^n:w_j=\\textrm{signe}(x_j)1\\textrm{ si }x_j\\neq 0, w_j\\in[-1,1]\\textrm{ si }x_j=0\\}.\n\\end{align*}\\]\n\n\nÉtant données \\(n\\) observations \\((x_i,y_i),i=1,\\dots,n\\) telles que \\(x_i\\in\\mathbb R^p\\) et \\(y_i\\in\\mathbb R\\) on rappelle que l’estimateur lasso \\(\\hat\\beta(\\lambda)\\) est construit en minimisant \\[\\mathcal L(\\beta)=\\|Y-\\mathbb X\\beta\\|_2^2+\\lambda\\|\\beta\\|_1. \\tag{3.1}\\]\nOn admettra que la sous-différentielle \\(\\partial \\mathcal L(\\beta)\\) est donnée par \\[\\partial \\mathcal L(\\beta)=\\left\\{-2\\mathbb X^t(Y-\\mathbb X\\beta)+\\lambda z:z\\in\\partial\\|\\beta\\|_1\\right\\}.\\] Montrer que \\(\\hat\\beta(\\lambda)\\) vérifie \\[\\mathbb X^t\\mathbb X\\hat\\beta(\\lambda)=\\mathbb X^tY-\\frac{\\lambda}{2}\\hat z\\] où \\(\\hat z\\in\\mathbb R^p\\) vérifie \\[\n\\hat z_j\\left\\{\n\\begin{array}{ll}\n=\\textrm{signe}(\\hat\\beta_j(\\lambda))   & \\textrm{si } \\hat\\beta_j(\\lambda)\\neq 0 \\\\\n\\in\\left[-1;1\\right] & \\textrm{sinon.}\n\\end{array}\\right.\n\\]\n\nD’après les indications, on a \\(0\\in\\partial \\mathcal L(\\hat\\beta(\\lambda))\\). Donc il existe \\(\\hat z\\in\\partial\\|\\hat\\beta(\\lambda)\\|_1\\) tel que \\[-2\\mathbb X^t(Y-\\mathbb X\\hat\\beta(\\lambda))+\\lambda \\hat z=0\\quad\\Longleftrightarrow\\quad \\mathbb X^t\\mathbb X\\hat\\beta(\\lambda)=\\mathbb X^tY-\\frac{\\lambda}{2}\\hat z.\\]\n\nOn suppose maintenant que la matrice \\(\\mathbb X\\) est orthogonale.\n\nMontrer que \\[\\textrm{signe}(\\hat\\beta_j(\\lambda))=\\textrm{signe}(\\mathbb X_j^tY)\\quad\\textrm{lorsque }\\hat\\beta_j(\\lambda)\\neq 0\\] et \\(\\hat\\beta_j(\\lambda)=0\\) si et seulement si \\(|\\mathbb X_j^tY|\\leq \\lambda/2\\).\n\n\\(\\mathbb X\\) étant orthogonale, on a pour \\(\\hat\\beta_j(\\lambda)\\neq 0\\) \\[\\hat\\beta_j(\\lambda)+\\frac{\\lambda}{2}\\textrm{signe}(\\hat\\beta_j(\\lambda))=\\hat\\beta_j(\\lambda)\\left(1+\\frac{\\lambda}{2|\\hat\\beta_j(\\lambda)|}\\right)=\\mathbb X_j^tY,\\] donc \\(\\hat\\beta_j(\\lambda)\\) est du signe de \\(\\mathbb X_j^tY\\). De plus si \\(\\hat\\beta_j(\\lambda)=0\\) alors \\(\\mathbb X^t_jY=\\frac{\\lambda}{2}\\hat z_j\\) avec \\(\\hat z_j\\in[-1,1]\\). Donc \\[|\\mathbb X^t_jY|=\\left|\\frac{\\lambda}{2}\\hat z_j\\right|\\leq\\frac{\\lambda}{2}.\\] A l’inverse si \\(|\\mathbb X_j^tY|\\leq \\lambda/2\\) et si \\(\\hat\\beta_j(\\lambda)\\neq 0\\) alors \\[\\left|\\hat\\beta_j(\\lambda)\\left(1+\\frac{\\lambda}{2|\\hat\\beta_j(\\lambda)|}\\right)\\right|=|\\hat\\beta_j(\\lambda)|+\\frac{\\lambda}{2}=|\\mathbb X^t_jY|\\leq\\frac{\\lambda}{2}.\\] Donc \\(\\hat\\beta_j(\\lambda)=0\\).\n\nEn déduire \\[\\hat\\beta_j(\\lambda)=\\mathbb X_j^tY\\left(1-\\frac{\\lambda}{2|\\mathbb X_j^tY|}\\right)_+,\\quad j=1,\\dots,p\\] où \\((x)_+=\\max(x,0)\\). Interpréter ce résultat.\n\nOn obtient donc \\[\\hat\\beta_j(\\lambda)=\\mathbb X_j^tY-\\frac{\\lambda}{2}\\,\\frac{\\mathbb X_j^tY}{|\\mathbb X_j^tY|}=\\mathbb X_j^tY\\left(1-\\frac{\\lambda}{2|\\mathbb X_j^tY|}\\right)\\] si \\(\\mathbb X_j^tY\\geq \\frac{\\lambda}{2}\\) et \\(\\hat\\beta_j(\\lambda)=0\\) sinon. D’où \\[\\hat\\beta_j(\\lambda)=\\mathbb X_j^tY\\left(1-\\frac{\\lambda}{2|\\mathbb X_j^tY|}\\right)_+,\\quad j=1,\\dots,d.\\]\n\n\n\n\n\n\nExercice 3.3 (Unicité de l’estimateur lasso) Cet exercice est inspiré de Giraud (2015). Soit \\(\\hat\\beta^{1}(\\lambda)\\) et \\(\\hat\\beta^{2}(\\lambda)\\) deux solutions qui minimisent l’Équation 3.1. Soit \\(\\hat\\beta=(\\hat\\beta^{1}(\\lambda)+\\hat\\beta^{2}(\\lambda))/2\\).\n\nMontrer que si \\(\\mathbb X \\hat\\beta^{1}(\\lambda)\\neq\\mathbb X \\hat\\beta^{2}(\\lambda)\\) alors \\[\\|\\mathbb Y-\\mathbb X\\hat\\beta\\|_2^2+\\lambda\\|\\hat\\beta\\|_1<\\frac{1}{2}\\left(\\|\\mathbb Y-\\mathbb X\\hat\\beta^1(\\lambda)\\|_2^2+\\lambda\\|\\hat\\beta^1(\\lambda)\\|_1+\\|\\mathbb Y-\\mathbb X\\hat\\beta^2(\\lambda)\\|_2^2+\\lambda\\|\\hat\\beta^2(\\lambda)\\|_1\\right).\\] On pourra utiliser la convexité (forte) de \\(x\\mapsto\\|x\\|_2^2\\).\n\nOn a\n\\[\\begin{align*}\n\\|\\mathbb Y-\\mathbb X\\hat\\beta\\|_2^2+\\lambda\\|\\hat\\beta\\|_1= & \\left\\|\\frac{1}{2}(\\mathbb Y-\\mathbb X\\hat\\beta^1(\\lambda))+\\frac{1}{2}(\\mathbb Y-\\mathbb X\\hat\\beta^2(\\lambda))\\right\\|_2^2+\\lambda\\left\\|\\frac{1}{2}(\\hat\\beta^1(\\lambda)+\\hat\\beta^2(\\lambda))\\right\\|_1 \\\\\n< & \\frac{1}{2}\\left\\|\\mathbb Y-\\mathbb X\\hat\\beta^1(\\lambda))\\right\\|_2^2+\\frac{1}{2}\\left\\|\\mathbb Y-\\mathbb X\\hat\\beta^2(\\lambda))\\right\\|_2^2+\\frac{1}{2}\\lambda\\|\\hat\\beta^1(\\lambda)\\|_1+\\frac{1}{2}\\lambda\\|\\hat\\beta^2(\\lambda)\\|_1\n\\end{align*}\\]\nen utilisant la stricte convexité de \\(x\\mapsto\\|x\\|_2^2\\) et l’inégalité triangulaire.\n\nEn déduire que \\(\\mathbb X \\hat\\beta^{1}(\\lambda)=\\mathbb X \\hat\\beta^{2}(\\lambda)\\).\n\nDonc si \\(\\mathbb X \\hat\\beta^{1}(\\lambda)\\neq\\mathbb X \\hat\\beta^{2}(\\lambda)\\) alors \\[\\|\\mathbb Y-\\mathbb X\\hat\\beta\\|_2^2+\\lambda\\|\\hat\\beta\\|_1< \\|\\mathbb Y-\\mathbb X\\hat\\beta^1(\\lambda)\\|_2^2+\\lambda\\|\\hat\\beta^1(\\lambda)\\|_1\\] ce qui est impossible par définition de \\(\\hat\\beta^1(\\lambda)\\).\n\n\n\n\n\n\n\nGiraud, C. 2015. Introduction to High-Dimensional Statistics. CRC Press."
  },
  {
    "objectID": "04-mod-add.html",
    "href": "04-mod-add.html",
    "title": "4  Modèle additif",
    "section": "",
    "text": "Le modèle additif (modèle GAM) peut être vu comme un compromis entre une modélisation linéaire et non paramétrique de la fonction de régression. Il suppose que cette fonction s’écrit \\[m(x)=m(x_1,\\dots,x_d)=\\alpha+g_1(x_1)+\\dots+g_d(x_d).\\]"
  },
  {
    "objectID": "04-mod-add.html#pseudo-backfitting",
    "href": "04-mod-add.html#pseudo-backfitting",
    "title": "4  Modèle additif",
    "section": "4.1 Pseudo backfitting",
    "text": "4.1 Pseudo backfitting\nL’algorithme du backfitting est souvent utilisé pour estimer les composantes du modèle additif. Etant donné un échantillon \\((x_i,y_i),i=1,\\dots,n\\) on note \\(\\bar{\\mathbb Y}\\) le vecteur des \\(y_i\\) et \\(\\mathbb X_k\\) le vecteur contenant les observations de la variable \\(k\\) pour \\(k=1,\\dots,d\\). L’algorithme se résume ainsi\n\n\nInitialisation : \\(\\widehat\\alpha=\\bar{\\mathbb Y}\\), \\(\\widehat g_k(x_k)=\\bar{\\mathbb X}_k\\).\nPour \\(k=1,\\dots,d\\) :\n\n\\(\\mathbb Y^{(k)}=\\mathbb Y-\\widehat\\alpha-\\sum_{j\\neq k}\\widehat g_j(\\mathbb X_j)\\) (résidus partiels)\n\\(\\widehat g_k\\) : lissage non paramétrique de \\(\\mathbb Y^{(k)}\\) sur \\(\\mathbb X_k\\).\n\nRépéter l’étape précédente tant que les \\(\\widehat g_k\\) changent.\n\n\nOn propose dans cette partie d’utiliser cet algorithme pour estimer les paramètres du modèle linéaire en remplaçant le lissage non paramétrique par un estimateur MCO. On considère le modèle de régression linéaire\n\\[Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\varepsilon\\] avec \\(X_1\\) et \\(X_2\\) de lois uniformes sur \\([0,1]\\) et \\(\\varepsilon\\) de loi \\(\\mathcal N(0,1)\\) (\\(\\varepsilon\\) est indépendante de \\((X_1,X_2)^\\prime\\)).\n\nGénérer un échantillon \\((x_i,y_i)\\) de taille \\(n=300\\) selon le modèle ci-dessus pour \\(\\beta_0=1,\\beta_1=3,\\beta_2=5\\).\n\n\n\nCréer une fonction R qui admet en entrée un jeu de données et qui fournit en sortie les estimateurs par la méthode du backfitting.\n\n\n\nEn déduire les estimateurs backfitting pour le problème considéré.\n\n\n\nComparer aux estimateurs MCO."
  },
  {
    "objectID": "04-mod-add.html#modèle-gam",
    "href": "04-mod-add.html#modèle-gam",
    "title": "4  Modèle additif",
    "section": "4.2 Modèle GAM",
    "text": "4.2 Modèle GAM\nOn considère les données générées selon\n\nn <- 1000\nset.seed(1465)\nX1 <- 2*runif(n)\nX2 <- 2*runif(n)\nbruit <- rnorm(n)\nY <- 2*X1+sin(8*pi*X2)+bruit\ndonnees<-data.frame(Y,X1,X2)\n\n\nÉcrire le modèle\n\nIl s’agit d’un modèle additif \\[Y=2X_1+\\sin(8\\pi X_2)+\\varepsilon\\] où \\(X_1\\) et \\(X_2\\) sont uniformes sur \\([0,1]\\) et \\(\\varepsilon\\) suit une \\(\\mathcal N(0,1)\\).\n\nA l’aide du package gam visualiser les estimateurs des composantes additives du modèle. On utilisera tout d’abord un lissage par spline avec 1 ddl pour la première composante et 24.579 ddl pour la seconde.\n\n\n\nFaire varier les degrés de liberté, interpréter.\n\nOn prend d’abord peu de degrés de liberté.\n\n\n\n\n\nLe sinus n’est pas bien estimé. On prend maintenant un grand nombre de degrés de liberté.\n\n\n\n\n\nLe modèle est trop flexible, risque de sur-ajustement.\n\nFaire le même travail avec le lisseur loess. On commencera avec degree=2 et span=0.15 puis on fera varier le paramètre span.\n\n\n\n\nOn fait varier span :\n\n\n\n\n\n\n\n\nOn a les mêmes remarques que pour les splines.\n\nEstimer le degrés de liberté avec la fonction gam du package mgcv (Il n’est pas nécessaire de charger le package pour éviter les conflits)."
  },
  {
    "objectID": "04-mod-add.html#régression-logistique-additive",
    "href": "04-mod-add.html#régression-logistique-additive",
    "title": "4  Modèle additif",
    "section": "4.3 Régression logistique additive",
    "text": "4.3 Régression logistique additive\nOn considère le jeu de données panne.txt qui recense des pannes de machine (etat=1) en fonction de leur âge et de leur marque.\n\nFaire une régression logistique permettant d’expliquer la variable etat par la variable age uniquement. Critiquer le modèle.\n\n\n\n\nLe modèle n’est pas pertinent. On accepte la nullité du coefficient age, ce qui signifie que le modèle constant est meilleur que le modèle avec la variable age.\n\nAjuster un modèle additif, toujours avec uniquement la variable age.\n\n\n\nEn utilisant le modèle additif, proposer un nouveau modèle logistique plus pertinent.\n\nIl semble que l’âge agisse de façon quadratique. Cela peut s’expliquer par le fait que les pannes interviennent souvent au début (phase de rodage) et à la fin (vieillissement de la machine).\n\n\n\n\n\nOn remarque ici que l’âge devient “significatif” !"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Références",
    "section": "",
    "text": "Giraud, C. 2015. Introduction to High-Dimensional Statistics.\nCRC Press."
  }
]
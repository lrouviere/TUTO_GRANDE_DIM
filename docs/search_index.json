[
["index.html", "Régression en grande dimension Présentation", " Régression en grande dimension Laurent Rouvière 2020-06-12 Présentation Ce tutoriel présente quelques exercices d’application du cours Modèle linéaire en grande dimension. On pourra trouver les documents de cours ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/stat_grand_dim/. Des connaissances de base en R sont nécessaires. Le tutoriel se structure en 4 parties : Fléau de la dimension : identification du problème de la dimension pour le problème de régression ; Régression sur composantes : présentation des algorithmes PCR et PLS ; Régression pénalisées: régularisation à l’aide de pénalités de type Ridge/Lasso Modèle additif : conservation de la structure additive du modèle linéaire mais modélisation non paramétrique des composantes.. "],
["intro-grande-dim.html", "Chapitre 1 Introduction à la grande dimension 1.1 Fléau de la dimension pour les plus proches voisins 1.2 Influence de la dimension dans le modèle linéaire", " Chapitre 1 Introduction à la grande dimension 1.1 Fléau de la dimension pour les plus proches voisins La fonction suivante permet de générer un échantillon d’apprentissage et un échantillon test selon le modèle \\[Y=X_1^2+\\dots+X_p^2+\\varepsilon\\] où les \\(X_j\\) sont uniformes i.i.d de loi uniorme sur \\([0,1]\\) et le bruit \\(\\varepsilon\\) suit une loi \\(\\mathcal N(0,0.5^2)\\). &gt; simu &lt;- function(napp=300,ntest=500,p=3,graine=1234){ + set.seed(graine) + n &lt;- napp+ntest + X &lt;- matrix(runif(n*p),ncol=p) + Y &lt;- apply(X^2,1,sum)+rnorm(n,sd=0.5) + Yapp &lt;- Y[1:napp] + Ytest &lt;- Y[-(1:napp)] + Xapp &lt;- data.frame(X[1:napp,]) + Xtest &lt;- data.frame(X[-(1:napp),]) + return(list(Xapp=Xapp,Yapp=Yapp,Xtest=Xtest,Ytest=Ytest)) + } &gt; df &lt;- simu(napp=300,ntest=500,p=3,graine=1234) La fonction knn.reg du package FNN permet de construire des estimateurs des \\(k\\) plus proches voisins en régression. On peut par exemple faire du 3 plus proches voisin avec &gt; library(FNN) &gt; mod3ppv &lt;- knn.reg(train=df$Xapp,y=df$Yapp,k=3) Parmi toutes les sorties proposées par cette fonction on a notamment &gt; mod3ppv$PRESS ## [1] 98.98178 qui renvoie la somme des carrés des erreurs de prévision par validation croisée Leave-one-out (LOO). On peut ainsi obtenir l’erreur quadratique moyenne LOO &gt; mod3ppv$PRESS/max(c(nrow(df$Xapp),1)) ## [1] 0.3299393 Construire la fonction sel.k qui admet en entrée : une grille de valeurs possibles de plus proches voisins (un vecteur). une matrice Xapp de dimension \\(n\\times p\\) qui contient les valeurs variables explicatives. un vecteur Yapp de dimension \\(n\\) qui contient les valeurs de la variable à expliquer et qui renvoie en sortie la valeur de \\(k\\) dans la grille qui minimise l’erreur LOO présentée ci-dessus. &gt; sel.k &lt;- function(K_cand=seq(1,50,by=5),Xapp,Yapp){ + ind &lt;- 1 + err &lt;- rep(0,length(K_cand)) + for (k in K_cand){ + modkppv &lt;- knn.reg(train=Xapp,y=Yapp,k=k) + err[ind] &lt;- modkppv$PRESS/max(c(nrow(Xapp),1)) + ind &lt;- ind+1 + } + return(K_cand[which.min(err)]) + } Une fois la fonction créée, on peut calculer l’erreur de l’estimateur sélectionné sur un échantillon test avec &gt; k.opt &lt;- sel.k(seq(1,50,by=5),df$Xapp,df$Yapp) &gt; prev &lt;- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred &gt; mean((prev-df$Ytest)^2) ## [1] 0.283869 On souhaite comparer les erreurs des règles des \\(k\\) plus proches voisins en fonction de la dimension. On considère les 4 dimensions et la grille de valeurs de \\(k\\) suivantes : &gt; DIM &lt;- c(1,5,10,50) &gt; K_cand &lt;- seq(1,50,by=5) Pour chaque valeur de dimension répéter \\(B=100\\) fois : simuler un échantillon d’apprentissage de taille 300 et test de taille 500 calculer la valeur optimale de \\(k\\) dans K_cand grâce à sel.k calculer l’erreur de l’estimateur sélectionné sur un échantillon test. On pourra stocker les résultats dans une matrice de dimension \\(B\\times 4\\). &gt; B &lt;- 100 &gt; mat.err &lt;- matrix(0,ncol=length(DIM),nrow=B) &gt; for (p in 1:length(DIM)){ + for (i in 1:B){ + df &lt;- simu(napp=300,ntest=500,p=DIM[p],graine=1234*p+2*i) + k.opt &lt;- sel.k(K_cand,df$Xapp,df$Yapp) + prev &lt;- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred + mat.err[i,p] &lt;- mean((prev-df$Ytest)^2) + } + } A l’aide d’indicateurs numériques et de boxplots, comparer la distribution des erreurs en fonction de la dimension. &gt; df &lt;- data.frame(mat.err) &gt; nom.dim &lt;- paste(&quot;D&quot;,DIM,sep=&quot;&quot;) &gt; names(df) &lt;- nom.dim &gt; df %&gt;% summarise_all(mean) ## D1 D5 D10 D50 ## 1 0.258003 0.3243574 0.52247 3.191055 &gt; df %&gt;% summarise_all(var) ## D1 D5 D10 D50 ## 1 0.0002556399 0.0005417109 0.001857967 0.06749414 &gt; df1 &lt;- gather(df,key=&quot;dim&quot;,value=&quot;erreur&quot;) &gt; df1 &lt;- df1 %&gt;% mutate(dim=fct_relevel(dim,nom.dim)) &gt; ggplot(df1)+aes(x=dim,y=erreur)+geom_boxplot()+theme_classic() Conclure Les estimateurs sont moins précis lorsque la dimension augmente. C’est le fléau de la dimension. 1.2 Influence de la dimension dans le modèle linéaire En vous basant sur l’exercice précédent, proposer une illustration qui peut mettre en évidence la précision d’estimation dans le modèle linéaire en fonction de la dimension. On considère un modèle linéaire avec une variable explicative \\(X_1\\) et \\(p-1\\) variables de bruit \\(X_2,\\dots,X_p\\). Par exemple \\[Y=X_1+0X_2+\\dots+0X_p+\\varepsilon.\\] On peut par exmple simuler des données issues de ce modèle avec la fonction suivante : &gt; n &lt;- 250 &gt; p &lt;- 1000 &gt; X &lt;- matrix(runif(n*p),ncol=p) &gt; simu.lin &lt;- function(X,graine){ + set.seed(graine) + Y &lt;- X[,1]+rnorm(nrow(X),sd=0.5) + df &lt;- data.frame(Y,X) + return(df) + } On s’intéresse à la distribution de \\(\\widehat\\beta_1\\) en fonction de la dimension. Pour ce faire, on calcule un grand nombre d’estimateurs de \\(\\widehat\\beta_1\\) pour différentes valeus de \\(p\\). &gt; B &lt;- 500 &gt; DIM &lt;- c(0,50,100,200) &gt; matbeta1 &lt;- matrix(0,nrow=B,ncol=length(DIM)) &gt; for (i in 1:B){ + dftot &lt;- simu.lin(X,i+1) + for (p in 1:length(DIM)){ + dfp &lt;- dftot[,(1:(2+DIM[p]))] + mod &lt;- lm(Y~.,data=dfp) + matbeta1[i,p] &lt;- coef(mod)[2] + } + } On compare, pour chaque dimension considérée, les distributions de \\(\\widehat\\beta_1\\). &gt; df &lt;- data.frame(matbeta1) &gt; nom.dim &lt;- paste(&quot;D&quot;,DIM,sep=&quot;&quot;) &gt; names(df) &lt;- nom.dim &gt; df %&gt;% summarise_all(mean) ## D0 D50 D100 D200 ## 1 0.992891 0.9960811 0.9959025 0.98173 &gt; df %&gt;% summarise_all(var) ## D0 D50 D100 D200 ## 1 0.01266578 0.016072 0.02023046 0.06939837 &gt; df1 &lt;- gather(df,key=&quot;dim&quot;,value=&quot;erreur&quot;) &gt; df1 &lt;- df1 %&gt;% mutate(dim=fct_relevel(dim,nom.dim)) &gt; ggplot(df1)+aes(x=dim,y=erreur)+geom_boxplot()+theme_classic() On retrouve bien que la dimension impacte notamment la variance des estimateurs. "],
["references.html", "References", " References "]
]

[
["index.html", "Régression en grande dimension Présentation", " Régression en grande dimension Laurent Rouvière 2020-10-23 Présentation Ce tutoriel présente quelques exercices d’application du cours Modèle linéaire en grande dimension. On pourra trouver les supports de cours associés à ce tutoriel ainsi que les données utilisées à l’adresse suivante https://lrouviere.github.io/stat_grand_dim/ ; le tutoriel sans les correction à l’url https://lrouviere.github.io/TUTO_GRANDE_DIM/ le tutoriel avec les corrigés (à certains moment) à l’url https://lrouviere.github.io/TUTO_GRANDE_DIM/correction. Il est recommandé d’utiliser mozilla firefox pour lire le tutoriel. Des connaissances de base en R et en statistique (modèles de régression) sont nécessaires. Le tutoriel se structure en 4 parties : Fléau de la dimension : identification du problème de la dimension pour le problème de régression ; Régression sur composantes : présentation des algorithmes PCR et PLS ; Régressions pénalisées: régularisation à l’aide de pénalités de type Ridge/Lasso Modèle additif : conservation de la structure additive du modèle linéaire mais modélisation non paramétrique des composantes. "],
["intro-grande-dim.html", "Chapitre 1 Introduction à la grande dimension 1.1 Fléau de la dimension pour les plus proches voisins 1.2 Influence de la dimension dans le modèle linéaire 1.3 Exercices", " Chapitre 1 Introduction à la grande dimension Nous proposons ici d’illustrer le problème de la grande dimension en régression. On commencera par étudier, à l’aide de simulation, ce problème pour l’estimateurs des \\(k\\) plus proches voisins, puis pour les estimateurs des moindres carrés dans le modèle linéaire. Quelques exercices sont ensuite proposées pour calculer les vitesses de convergence de ces estimateurs dans des modèles simples. 1.1 Fléau de la dimension pour les plus proches voisins La fonction suivante permet de générer un échantillon d’apprentissage et un échantillon test selon le modèle \\[Y=X_1^2+\\dots+X_p^2+\\varepsilon\\] où les \\(X_j\\) sont uniformes i.i.d de loi uniorme sur \\([0,1]\\) et le bruit \\(\\varepsilon\\) suit une loi \\(\\mathcal N(0,0.5^2)\\). simu &lt;- function(napp=300,ntest=500,p=3,graine=1234){ set.seed(graine) n &lt;- napp+ntest X &lt;- matrix(runif(n*p),ncol=p) Y &lt;- apply(X^2,1,sum)+rnorm(n,sd=0.5) Yapp &lt;- Y[1:napp] Ytest &lt;- Y[-(1:napp)] Xapp &lt;- data.frame(X[1:napp,]) Xtest &lt;- data.frame(X[-(1:napp),]) return(list(Xapp=Xapp,Yapp=Yapp,Xtest=Xtest,Ytest=Ytest)) } df &lt;- simu(napp=300,ntest=500,p=3,graine=1234) La fonction knn.reg du package FNN permet de construire des estimateurs des \\(k\\) plus proches voisins en régression. On peut par exemple faire du 3 plus proches voisins avec library(FNN) mod3ppv &lt;- knn.reg(train=df$Xapp,y=df$Yapp,k=3) Parmi toutes les sorties proposées par cette fonction on a notamment mod3ppv$PRESS [1] 98.98178 qui renvoie la somme des carrés des erreurs de prévision par validation croisée Leave-One-Out (LOO). On peut ainsi obtenir l’erreur quadratique moyenne par LOO mod3ppv$PRESS/max(c(nrow(df$Xapp),1)) [1] 0.3299393 Construire la fonction sel.k qui admet en entrée : une grille de valeurs possibles de plus proches voisins (un vecteur). une matrice Xapp de dimension \\(n\\times p\\) qui contient les valeurs variables explicatives. un vecteur Yapp de dimension \\(n\\) qui contient les valeurs de la variable à expliquer et qui renvoie en sortie la valeur de \\(k\\) dans la grille qui minimise l’erreur LOO présentée ci-dessus. sel.k &lt;- function(K_cand=seq(1,50,by=5),Xapp,Yapp){ ind &lt;- 1 err &lt;- rep(0,length(K_cand)) for (k in K_cand){ modkppv &lt;- knn.reg(train=Xapp,y=Yapp,k=k) err[ind] &lt;- modkppv$PRESS/max(c(nrow(Xapp),1)) ind &lt;- ind+1 } return(K_cand[which.min(err)]) } Une fois la fonction créée, on peut calculer l’erreur de l’estimateur sélectionné sur un échantillon test avec k.opt &lt;- sel.k(seq(1,50,by=5),df$Xapp,df$Yapp) prev &lt;- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred mean((prev-df$Ytest)^2) [1] 0.283869 On souhaite comparer les erreurs des règles des \\(k\\) plus proches voisins en fonction de la dimension. On considère 4 dimensions collectées dans le vecteur DIM et la grille de valeurs de \\(k\\) suivantes : DIM &lt;- c(1,5,10,50) K_cand &lt;- seq(1,50,by=5) Pour chaque valeur de dimension répéter \\(B=100\\) fois : simuler un échantillon d’apprentissage de taille 300 et test de taille 500 calculer la valeur optimale de \\(k\\) dans K_cand grâce à sel.k calculer l’erreur de l’estimateur sélectionné sur un échantillon test. On pourra stocker les résultats dans une matrice de dimension \\(B\\times 4\\). B &lt;- 100 mat.err &lt;- matrix(0,ncol=length(DIM),nrow=B) for (p in 1:length(DIM)){ for (i in 1:B){ df &lt;- simu(napp=300,ntest=500,p=DIM[p],graine=1234*p+2*i) k.opt &lt;- sel.k(K_cand,df$Xapp,df$Yapp) prev &lt;- knn.reg(train=df$Xapp,y=df$Yapp,test=df$Xtest,k=k.opt)$pred mat.err[i,p] &lt;- mean((prev-df$Ytest)^2) } } A l’aide d’indicateurs numériques et de boxplots, comparer la distribution des erreurs en fonction de la dimension. df &lt;- data.frame(mat.err) nom.dim &lt;- paste(&quot;D&quot;,DIM,sep=&quot;&quot;) names(df) &lt;- nom.dim df %&gt;% summarise_all(mean) D1 D5 D10 D50 1 0.258003 0.3243574 0.52247 3.191055 df %&gt;% summarise_all(var) D1 D5 D10 D50 1 0.0002556399 0.0005417109 0.001857967 0.06749414 df1 &lt;- pivot_longer(df,cols=everything(),names_to=&quot;dim&quot;,values_to=&quot;erreur&quot;) df1 &lt;- df1 %&gt;% mutate(dim=fct_relevel(dim,nom.dim)) ggplot(df1)+aes(x=dim,y=erreur)+geom_boxplot() Conclure Les estimateurs sont moins précis lorsque la dimension augmente. C’est le fléau de la dimension. 1.2 Influence de la dimension dans le modèle linéaire En vous basant sur l’exercice précédent, proposer une illustration qui peut mettre en évidence la précision d’estimation dans le modèle linéaire en fonction de la dimension. On pourra par exemple considérer le modèle linaire suivant \\[Y=X_1+0X_2+\\dots+0X_p+\\varepsilon\\] et étudier la performance de l’estimateur MCO du coefficient de \\(X_1\\) pour différentes valeurs de \\(p\\). Par exemple avec \\(p\\) dans le vecteur DIM &lt;- c(0,50,100,200) Les données pourront être générées avec la fonction suivante n &lt;- 250 p &lt;- 1000 X &lt;- matrix(runif(n*p),ncol=p) simu.lin &lt;- function(X,graine){ set.seed(graine) Y &lt;- X[,1]+rnorm(nrow(X),sd=0.5) df &lt;- data.frame(Y,X) return(df) } On s’intéresse à la distribution de \\(\\widehat\\beta_1\\) en fonction de la dimension. Pour ce faire, on calcule un grand nombre d’estimateurs de \\(\\widehat\\beta_1\\) pour différentes valeurs de \\(p\\). B &lt;- 500 matbeta1 &lt;- matrix(0,nrow=B,ncol=length(DIM)) for (i in 1:B){ dftot &lt;- simu.lin(X,i+1) for (p in 1:length(DIM)){ dfp &lt;- dftot[,(1:(2+DIM[p]))] mod &lt;- lm(Y~.,data=dfp) matbeta1[i,p] &lt;- coef(mod)[2] } } On met en forme les résultats df &lt;- data.frame(matbeta1) nom.dim &lt;- paste(&quot;D&quot;,DIM,sep=&quot;&quot;) names(df) &lt;- nom.dim Puis on compare, pour chaque dimension considérée, les distributions de \\(\\widehat\\beta_1\\) : en étudiant le biais et la variance df %&gt;% summarise_all(mean) D0 D50 D100 D200 1 0.992891 0.9960811 0.9959025 0.98173 df %&gt;% summarise_all(var) D0 D50 D100 D200 1 0.01266578 0.016072 0.02023046 0.06939837 en visualisant la distribution avec un boxplot df1 &lt;- gather(df,key=&quot;dim&quot;,value=&quot;erreur&quot;) df1 &lt;- df1 %&gt;% mutate(dim=fct_relevel(dim,nom.dim)) ggplot(df1)+aes(x=dim,y=erreur)+geom_boxplot()+theme_classic() On retrouve bien que la dimension impacte notamment la variance des estimateurs. 1.3 Exercices Exercice 1.1 (Distances entre deux points, voir Giraud (2015)) Soit \\(X^{(1)}=(X_1^{(1)},\\dots,X_p^{(1)})\\) et \\(X^{(2)}=(X_1^{(2)},\\dots,X_p^{(2)})\\) deux variables aléatoires indépendantes de loi uniforme sur l’hypercube \\([0,1]^p\\). Montrer que \\[\\mathbf E[\\|X^{(1)}-X^{(2)}\\|^2]=\\frac{p}{6}\\quad\\text{et}\\quad\\sigma[\\|X^{(1)}-X^{(2)}\\|^2]\\approx 0.2\\sqrt{p}.\\] Soit \\(U\\) et \\(U^\\prime\\) deux variables aléatoires indépendantes de loi uniforme sur \\([0,1]\\). On a \\[\\mathbf E[\\|X^{(1)}-X^{(2)}\\|^2]=\\sum_{k=1}^p\\mathbf E\\left[\\left(X_k^{(1)}-X_k^{(2)}\\right)\\right]=p\\mathbf E[(U-U^\\prime)^2]=p(2\\mathbf E[U^2]-2\\mathbf E[U]^2)=\\frac{p}{6}\\] car \\(\\mathbf E[U^2]=1/3\\) et \\(\\mathbf E[U]=1/2\\). De même \\[\\sigma[\\|X^{(1)}-X^{(2)}\\|^2]=\\sqrt{\\sum_{k=1}^p\\mathbf V\\left[\\left(X_k^{(1)}-X_k^{(2)}\\right)\\right]}=\\sqrt{p\\mathbf V[(U^\\prime-U)^2]}\\approx 0.2\\sqrt{p}\\] car \\[\\mathbf E\\left[(U^\\prime-U)^4\\right]=\\int_0^1\\int_0^1(x-y)^4\\,\\mathrm{d}x\\mathrm{d}y=\\frac{1}{15}\\] et donc \\(\\mathbf V[(U^\\prime-U)^2]=1/15-1/36\\approx 0.04\\). Exercice 1.2 (Vitesse de convergence pour l’estimateur à noyau) On considère le modèle de régression \\[Y_i=m(x_i)+\\varepsilon_i,\\quad i=1,\\dots,n\\] où \\(x_1,\\dots,x_n\\in\\mathbb R^d\\) sont déterministes et \\(\\varepsilon_1,\\dots,\\varepsilon_n\\) sont des variables i.i.d. d’espérance nulle et de variance \\(\\sigma^2&lt;+\\infty\\). On désigne par \\(\\|\\,.\\,\\|\\) la norme Euclidienne dans \\(\\mathbb R^d\\). On définit l’estimateur localement constant de \\(m\\) en \\(x\\in\\mathbb R^d\\) par : \\[\\hat m(x)=\\mathop{\\mathrm{argmin}}_{a\\in\\mathbb R}\\sum_{i=1}^n(Y_i-a)^2K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] où \\(h&gt;0\\) et pour \\(u\\in\\mathbb R,K(u)=\\mathbf 1_{[0,1]}(u)\\). On suppose que \\(\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)&gt;0\\). Donner la forme explicite de \\(\\hat m(x)\\). En annulant la dérivée par rapport à \\(a\\), on obtient \\[\\hat m(x)=\\frac{\\sum_{i=1}^nY_iK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\] Montrer que \\[\\mathbf V[\\hat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}\\] et \\[\\mathbf E[\\hat m(x)]-m(x)=\\frac{\\sum_{i=1}^n(m(x_i)-m(x))K\\left(\\frac{\\|x_i-x\\|}{h}\\right)}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}.\\] Ces propriétés se déduisent directement en remarquant que \\(\\mathbf V[Y_i]=\\sigma^2\\) et \\(\\mathbf E[Y_i]=m(x_i)\\). On suppose maintenant que \\(m\\) est Lipschitzienne de constante \\(L\\), c’est-à-dire que \\(\\forall (x_1,x_2)\\in\\mathbb R^d\\times\\mathbb R^d\\) \\[|m(x_1)-m(x_2)|\\leq L\\|x_1-x_2\\|.\\] Montrer que \\[|\\textrm{biais}[\\hat m(x)]|\\leq Lh.\\] On a \\(|m(x_i)-m(x)|\\leq L\\|x_i-x\\|\\). Or \\[K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\] est non nul si et seulement si \\(\\|x_i-x\\|\\leq h\\). Donc pour tout \\(i=1,\\dots,n\\) \\[L\\|x_i-x\\|K\\left(\\frac{\\|x_i-x\\|}{h}\\right)\\leq Lh K\\left(\\frac{\\|x_i-x\\|}{h}\\right).\\] D’où le résultat. On suppose de plus qu’il existe une constante \\(C_1\\) telle que \\[C_1\\leq\\frac{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}{n\\textrm{Vol}(B_h)},\\] où \\(B_h=\\{u\\in\\mathbb R^d:\\|u\\|\\leq h\\}\\) est la boule de rayon \\(h\\) dans \\(\\mathbb R^d\\) et \\(\\textrm{Vol}(A)\\) désigne le volume d’un ensemble \\(A\\subset\\mathbb R^d\\). Montrer que \\[\\mathbf V[\\hat m(x)]\\leq\\frac{C_2\\sigma^2}{nh^d},\\] où \\(C_2\\) est une constante dépendant de \\(C_1\\) et \\(d\\) à préciser. On a \\[\\mathbf V[\\hat m(x)]= \\frac{\\sigma^2}{\\sum_{i=1}^nK\\left(\\frac{\\|x_i-x\\|}{h}\\right)}=\\frac{\\sigma^2}{\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)}.\\] Or \\[\\sum_{i=1}^n\\mathbf 1_{B_h}(x_i-x)\\geq C_1n\\textrm{Vol}(B_h)\\geq C_1\\gamma_dnh^d\\] où \\(\\gamma_d\\) désigne le volume de la boule unité en dimension \\(d\\). On a donc \\[\\mathbf V[\\hat m(x)]\\leq \\frac{\\sigma^2}{C_1\\gamma_dnh^d}.\\] Déduire des questions précédentes un majorant de l’erreur quadratique moyenne de \\(\\hat m(x)\\). On déduit \\[\\mathbf E[(\\hat m(x)-m(x))^2]\\leq L^2h^2+\\frac{C_2\\sigma^2}{nh^d}.\\] Calculer \\(h_{\\text{opt}}\\) la valeur de \\(h\\) qui minimise ce majorant. Que vaut ce majorant lorsque \\(h=h_{\\text{opt}}\\) ? Comment varie cette vitesse lorsque \\(d\\) augmente ? Interpréter. Soit \\(M(h)\\) le majorant. On a \\[M(h)^\\prime=2hL^2-\\frac{C_2\\sigma^2d}{n}h^{-d-1}.\\] La dérivée s’annule pour \\[h_{\\text{opt}}=\\frac{2L^2}{C_2\\sigma^2d}n^{-\\frac{1}{d+2}}.\\] Lorsque \\(h=h_{\\text{opt}}\\) l’erreur quadratique vérifie \\[\\mathbf E[(\\hat m(x)-m(x))^2]=\\mathrm{O}\\left(n^{-\\frac{2}{d+2}}\\right).\\] Références "],
["reg-comp.html", "Chapitre 2 Régression sur composantes 2.1 Sélection de variables 2.2 Régression sur composantes principales (méthodo) 2.3 Régression PLS : méthodo 2.4 Comparaison : PCR vs PLS.", " Chapitre 2 Régression sur composantes Les performances des estimateurs classiques (MCO) des paramètres du modèle linéaire \\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_dX_d+\\varepsilon\\] peuvent se dégrader lorsque la dimension \\(d\\) est grande ou en présence de dépendance linéaire entre les variables explicatives. Les régressions sur composantes consistent à trouver de nouvelles composantes \\(Z_k,j=k,\\dots,q\\) avec \\(q\\leq p\\) qui s’écrivent le plus souvent comme des combinaisons linéaires des \\(X_j\\) dans l’idée de diminuer le nombre de paramètres du modèle ou la dépendance entre les covariables. Il existe plusieurs façons de construire ces composantes, dans cette partie nous proposons : la régression sous composantes principales (PCR) : il s’agit de faire simplement une ACP sur la matrice des variables explicatives ; la régression partial least square (PLS) qui fait intervenir la variable cible dans la construction des composantes. Nous commençons par un bref rappel sur la sélection de variables. 2.1 Sélection de variables On considère le jeu de données ozone.txt où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable maxO3) par d’autres variables essentiellement météorologiques. ozone &lt;- read.table(&quot;data/ozone.txt&quot;) head(ozone) maxO3 T9 T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 20010601 87 15.6 18.5 18.4 4 4 8 0.6946 -1.7101 20010602 82 17.0 18.4 17.7 5 5 7 -4.3301 -4.0000 20010603 92 15.3 17.6 19.5 2 5 4 2.9544 1.8794 20010604 114 16.2 19.7 22.5 1 1 0 0.9848 0.3473 20010605 94 17.4 20.5 20.4 8 8 7 -0.5000 -2.9544 20010606 80 17.7 19.8 18.3 6 6 7 -5.6382 -5.0000 Vx15 maxO3v vent pluie 20010601 -0.6946 84 Nord Sec 20010602 -3.0000 87 Nord Sec 20010603 0.5209 82 Est Sec 20010604 -0.1736 92 Nord Sec 20010605 -4.3301 114 Ouest Sec 20010606 -6.0000 94 Ouest Pluie Ajuster un modèle linéaire avec lm et analyser la pertinence des variables explicatives dans le modèle. Expliquer les sorties de la commande library(leaps) mod.sel &lt;- regsubsets(maxO3~.,data=ozone,nvmax=14) summary(mod.sel) Subset selection object Call: regsubsets.formula(maxO3 ~ ., data = ozone, nvmax = 14) 14 Variables (and intercept) Forced in Forced out T9 FALSE FALSE T12 FALSE FALSE T15 FALSE FALSE Ne9 FALSE FALSE Ne12 FALSE FALSE Ne15 FALSE FALSE Vx9 FALSE FALSE Vx12 FALSE FALSE Vx15 FALSE FALSE maxO3v FALSE FALSE ventNord FALSE FALSE ventOuest FALSE FALSE ventSud FALSE FALSE pluieSec FALSE FALSE 1 subsets of each size up to 14 Selection Algorithm: exhaustive T9 T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 Vx15 maxO3v 1 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; 2 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; 3 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; 4 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; 5 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; 6 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; 10 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; 11 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 12 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ventNord ventOuest ventSud pluieSec 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; 7 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 9 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 10 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 11 ( 1 ) &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; Sélectionner le meilleur modèle au sens du \\(R^2\\). Que remarquez-vous ? Faire de même pour le \\(C_p\\) et le \\(BIC\\). Que remarquez-vous pour les variables explicatives qualitatives ? Comparer cette méthode avec des modèles sélectionnées par la fonction step ou la fonction bestglm du package bestglm. 2.2 Régression sur composantes principales (méthodo) On considère le jeu de données Hitters dans lequel on souhaite expliquer la variable Salary par les autres variables du jeu de données. Pour simplifier le problème, on supprime les individus qui possèdent des données manquantes (il ne faut pas faire ça normalement !). library(ISLR) Hitters &lt;- na.omit(Hitters) Parmi les variables explicatives, certaines sont qualitatives. Expliquer comment, à l’aide de la fonction model.matrix on peut utiliser ces variables dans un modèle linéaire. On appellera X la matrice des variables explicatives construites avec cette variable. Calculer la matrice Xcr qui correspond à la matrice X centrée réduite. On pourra utiliser la fonction scale. A l’aide de la fonction PCA du package FactoMineR, effectuer l’ACP du tableau Xcr avec l’option scale.unit=FALSE. Récupérer les coordonnées des individus sur les 5 premiers axes de l’ACP (variables \\(Z\\) dans le cours). Effectuer la régression linéaire sur les 5 premières composantes principales et calculer les estimateurs des MCO (\\(\\widehat\\theta_k,k=1,\\dots,5\\) dans le cours). En déduire les estimateurs dans l’espace des données initiales pour les données centrées réduites, puis pour les données brutes. On pourra récupérer les vecteurs propres de l’ACP (\\(u_k\\) dans le cours) dans la sortie svd de la fonction PCA Retrouver les estimateurs dans l’espace des données initiales pour les données centrées réduites à l’aide de la fonction pcr du package pls. On considère les individus suivants df.new &lt;- Hitters[c(1,100,80),] Calculer de 3 façons différentes les valeurs de salaire prédites par la régression sur 5 composantes principales. 2.3 Régression PLS : méthodo On considère les mêmes données que précédemment. A l’aide du vecteur \\(Y\\) (Salary) et de la matrice des \\(X\\) centrées réduites calculées dans l’exercice précédent, calculer la première composante PLS \\(Z_1\\). En déduire le coefficient associé à cette première composante en considérant le modèle \\[Y=\\alpha_1 Z_1+\\varepsilon.\\] En déduire les coefficients en fonction des variables initiales (centrées réduites) de la régression PLS à une composante \\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_pX_p+\\varepsilon.\\] Retrouver ces coefficients en utilisant la fonction plsr. 2.4 Comparaison : PCR vs PLS. Séparer le jeu de données (Hitters toujours) en un échantillon d’apprentissage de taille 200 et un échantillon test de taille 63. Avec les données d’apprentissage uniquement construire les régressions PCR et PLS. On choisira les nombres de composantes par validation croisée. Comparer les deux méthodes en utilisant l’échantillon de validation. On pourra également utiliser un modèle linéaire classique. Comparer ces méthodes à l’aide d’une validation croisée 10 blocs. "],
["reg-pen.html", "Chapitre 3 Régressions pénalisées (ou sous contraintes) 3.1 Ridge et lasso avec glmnet 3.2 Reconstruction d’un signal 3.3 Régression logistique pénalisée 3.4 Exercices", " Chapitre 3 Régressions pénalisées (ou sous contraintes) Nous considérons toujours le modèle linéaire \\[Y=\\beta_0+\\beta_1X_1+\\dots+\\beta_dX_d+\\varepsilon\\] Lorsque \\(d\\) est grand ou que les variables sont linéairement dépendantes, les estimateurs des moindres carrées peuvent être mis en défaut. Les méthodes pénalisées ou sous contraintes consistent alors à restreindre l’espace sur lequel on minimise ce critère. On va alors chercher le vecteur \\(\\beta\\) qui minimise \\[\\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^dx_{ij}\\beta_j\\right)^2\\quad\\text{sous la contrainte }\\quad\\sum_{j=1}^d\\beta_j^2\\leq t\\] ou de façon équivalente (dans le sens où il existe une équivalence entre \\(t\\) et \\(\\lambda\\)) \\[\\sum_{i=1}^n \\left(y_i-\\beta_0-\\sum_{j=1}^dx_{ij}\\beta_j\\right)^2+\\lambda\\sum_{j=1}^d\\beta_j^2.\\] Les estimateurs obtenus sont les estimateurs ridge. Les estimateurs lasso s’obtiennent en remplaçant la contrainte ou la pénalité par une norme 1 (\\(\\sum_{j=1}^d|\\beta_j|\\)). Nous présentons dans cette partie les étapes principales qui permettent de faire ce type de régression avec R. Le package le plus souvent utilisé est glmnet. 3.1 Ridge et lasso avec glmnet On considère le jeu de données ozone.txt où on cherche à expliquer la concentration maximale en ozone relevée sur une journée (variable maxO3) par d’autres variables essentiellement météorologiques. ozone &lt;- read.table(&quot;data/ozone.txt&quot;) head(ozone) maxO3 T9 T12 T15 Ne9 Ne12 Ne15 Vx9 Vx12 20010601 87 15.6 18.5 18.4 4 4 8 0.6946 -1.7101 20010602 82 17.0 18.4 17.7 5 5 7 -4.3301 -4.0000 20010603 92 15.3 17.6 19.5 2 5 4 2.9544 1.8794 20010604 114 16.2 19.7 22.5 1 1 0 0.9848 0.3473 20010605 94 17.4 20.5 20.4 8 8 7 -0.5000 -2.9544 20010606 80 17.7 19.8 18.3 6 6 7 -5.6382 -5.0000 Vx15 maxO3v vent pluie 20010601 -0.6946 84 Nord Sec 20010602 -3.0000 87 Nord Sec 20010603 0.5209 82 Est Sec 20010604 -0.1736 92 Nord Sec 20010605 -4.3301 114 Ouest Sec 20010606 -6.0000 94 Ouest Pluie Contrairement à la plupart des autres package R qui permettent de faire de l’apprentissage, le package glmnet n’autorise pas l’utilisation de formules : il faut spécifier explicitement la matrice des \\(X\\) et le vecteur des \\(Y\\). On peut obtenir la matrice des \\(X\\) et notamment le codage des variables qualitatives avec la fonction model.matrix: ozone.X &lt;- model.matrix(maxO3~.,data=ozone)[,-1] ozone.Y &lt;- ozone$maxO3 Charger le package glmnet et à l’aide de la fonction glmnet calculer les estimateurs ridge et lasso. Analyser les sorties qui se trouvent dans les arguments lambda et beta de glmnet. Visualiser les chemins de régularisation des estimateurs ridge et lasso. On pourra utiliser la fonction plot. Sélectionner les paramètres de régularisation à l’aide de la fonction cv.glmnet. On pourra notamment faire un plot de l’objet et expliquer le graphe obtenu. On souhaite prédire la variable cible pour de nouveaux individus, par exemple les 25ème et 50ème individus du jeu de données. Calculer les valeurs prédites pour ces deux individus. A l’aide d’une validation croisée, comparer les performances des estimateurs MCO, ridge et lasso. On pourra utiliser les données ozone_complet.txt qui contiennent plus d’individus et de variables. ozone1 &lt;- read.table(&quot;data/ozone_complet.txt&quot;,sep=&quot;;&quot;) %&gt;% na.omit() ozone1.X &lt;- model.matrix(maxO3~.,data=ozone1)[,-1] ozone1.Y &lt;- ozone1$maxO3 Refaire la question précédente en considérant toutes les interactions d’ordre 2. 3.2 Reconstruction d’un signal Le fichier signal.csv contient un signal que l’on peut représenter par une fonction \\(m:\\mathbb R\\to\\mathbb R\\). On le visualise signal &lt;- read_csv(&quot;data/signal.csv&quot;) ggplot(signal)+aes(x=x,y=y)+geom_line() Plaçons nous dans le cas où on ne dispose que d’une version bruitée de ce signal. La courbe n’est pas observée mais on dispose d’un échantillon \\((x_i,y_i),i=1,\\dots,n\\) généré selon le modèle \\[y_i=m(x_i)+\\varepsilon_i.\\] Le fichier ech_signal.csv contient \\(n=60\\) observations issues de ce modèle. On représente les données et la courbe donnees &lt;- read_csv(&quot;data/ech_signal.csv&quot;) ggplot(signal)+aes(x=x,y=y)+geom_line()+ geom_point(data=donnees,aes(x=X,y=Y)) Nous cherchons dans cette partie à reconstruire le signal à partir de l’échantillon. Bien entendu, vu la forme du signal, un modèle linéaire de la forme \\[y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i\\] n’est pas approprié. De nombreuses approches en traitement du signal proposent d’utiliser une base ou dictionnaire représentée par une collection de fonctions \\(\\{\\psi_j(x)\\}_{j=1,\\dots,K}\\) et de décomposer le signal dans cette base : \\[m(x)\\approx \\sum_{j=1}^K \\beta_j\\psi_j(x).\\] Pour un dictionnaire donné, on peut alors considérer un modèle linéaire \\[\\begin{equation} y_i=\\sum_{j=1}^K \\beta_j\\psi_j(x)+\\varepsilon_i. \\tag{3.1} \\end{equation}\\] Le problème est toujours d’estimer les paramètres \\(\\beta_j\\) mais les variables sont maintenant définies par les élements du dictionnaire. Il existe différents types de dictionnaire, dans cet exercice nous proposons de considérer la base de Fourier définie par \\[\\psi_0(x)=1,\\quad \\psi_{2j-1}(x)=\\cos(2j\\pi x)\\quad\\text{et}\\quad \\psi_{2j}(x)=\\sin(2j\\pi x),\\quad j=1,\\dots,K.\\] Écrire une fonction R qui admet en entrée : une grille de valeurs de x (un vecteur) une valeur de K (un entier positif) et qui renvoie en sortie une matrice qui contiennent les valeurs du dictionnaire pour chaque valeur de x. Cette matrice devra donc contenir 2K colonnes et le nombre de lignes sera égal à la longueur du vecteur x. On fixe K=25. Calculer les estimateurs des moindres carrés du modèle (3.1). Représenter le signal estimé. Commenter le graphe. Calculer les estimateurs lasso et représenter le signal issu de ces estimateurs. Identifier les coefficients lasso sélectionnés qui ne sont pas nuls. Ajouter les signaux ajustés par les algorithme PCR et PLS. 3.3 Régression logistique pénalisée On considère le jeu de données sur la détection d’images publicitaires disponible ici https://archive.ics.uci.edu/ml/datasets/internet+advertisements. ad.data &lt;- read.table(&quot;data/ad_data.txt&quot;,header=FALSE,sep=&quot;,&quot;,dec=&quot;.&quot;,na.strings = &quot;?&quot;,strip.white = TRUE) names(ad.data)[ncol(ad.data)] &lt;- &quot;Y&quot; ad.data$Y &lt;- as.factor(ad.data$Y) La variable à expliquer est summary(ad.data$Y) ad. nonad. 459 2820 Cette variable est binaire. On considère une régression logistique pour expliquer cette variable. Le nombre de variables explicatives étant important, comparer les algorithmes du maximum de vraisemblance aux algorithmes de type ridge/lasso en faisant une validation croisée 10 blocs. On pourra utiliser comme critère de comparaison l’erreur de classification, la courbe ROC et l’AUC. Il faudra également prendre des décisions pertinentes vis-à-vis des données manquantes… 3.4 Exercices Exercice 3.1 (Estimateurs ridge pour le modèle linéaire) On considère le modèle de régression \\[Y_i=\\beta_1x_{i1}+\\dots+\\beta_px_{ip}+\\varepsilon_i\\] où les \\(\\varepsilon_i\\) sont i.i.d de loi \\(\\mathcal N(0,\\sigma^2)\\). Pour \\(\\lambda\\geq 0\\), on note \\(\\hat\\beta_R(\\lambda)\\) l’estimateur ridge défini par \\[\\hat\\beta_R(\\lambda)=\\mathop{\\mathrm{argmin}}_\\beta\\sum_{i=1}^n\\left(y_i-\\sum_{j=1}^px_{ij}\\beta_j\\right)^2+\\lambda\\sum_{j=1}^p\\beta_j^2.\\] Exprimer \\(\\hat\\beta_R(\\lambda)\\) en fonction de \\(\\mathbb X\\), \\(\\mathbb Y\\) et \\(\\lambda\\). Étudier le biais et la variance de \\(\\hat\\beta_R(\\lambda)\\) en fonction de \\(\\lambda\\). On pourra également faire la comparaison avec l’estimateur des MCO. On suppose que la matrice \\(\\mathbb X\\) est orthogonale. Exprimer les estimateurs \\(\\hat\\beta_{R,j}(\\lambda)\\) en fonction des estimateurs des MCO \\(\\hat\\beta_j, j=1,\\dots,p\\). Interpréter. Exercice 3.2 (Estimateurs lasso dans le cas orthogonal,voir Giraud (2015)) On rappelle qu’une fonction \\(F:\\mathbb R^n\\to\\mathbb R\\) est convexe si \\(\\forall x,y\\in\\mathbb R^n\\), \\(\\forall\\lambda\\in[0,1]\\) on a \\[F(\\lambda x+(1-\\lambda) y)\\leq \\lambda F(x)+(1-\\lambda)F(y).\\] On définit la sous-différentielle d’une fonction convexe \\(F\\) par \\[\\partial F(x)=\\{w\\in\\mathbb R^n:F(y)\\geq F(x)+\\langle w,y-x\\rangle\\textrm{ pour tout }y\\in\\mathbb R^n\\}.\\] On admettra que les minima d’une fonction convexe \\(F:\\mathbb R^n\\to\\mathbb R\\) sont caractérisés par \\[x^\\star\\in\\mathop{\\mathrm{argmin}}_{x\\in\\mathbb R^n}F(x)\\Longleftrightarrow 0\\in \\partial F(x^\\star)\\] et que \\(\\partial F(x)=\\{\\nabla F(x)\\}\\) lorsque \\(F\\) est différentiable en \\(x\\). Montrer que pour \\(x\\in\\mathbb R\\) \\[ \\partial |x|=\\left\\{ \\begin{array}{ll} \\textrm{signe}(x) &amp; \\textrm{si } x\\neq 0 \\\\ \\left[-1;1\\right] &amp; \\textrm{sinon,} \\end{array}\\right. \\] où \\(\\text{signe}(x)=\\mathbf 1_{x&gt;0}-\\mathbf 1_{x\\leq 0}\\). Soit \\(x\\in\\mathbb R^n\\). Montrer que \\[\\partial\\|x\\|_1=\\{w\\in\\mathbb R^n:\\langle w,x\\rangle=\\|x\\|_1\\text{ et }\\|w\\|_\\infty\\leq 1\\}.\\] On pourra utiliser que pour tout \\(p,q\\) tels que \\(1/p+1/q=1\\) on a \\[\\|x\\|_p=\\sup\\left\\{\\langle w,x\\rangle:\\|w\\|_q\\leq 1\\right\\}.\\] En déduire \\[\\partial\\|x\\|_1=\\{w\\in\\mathbb R^n:w_j=\\textrm{signe}(x_j)\\textrm{ si }x_j\\neq 0, w_j\\in[-1,1]\\textrm{ si }x_j=0\\}.\\] Étant données \\(n\\) observations \\((x_i,y_i),i=1,\\dots,n\\) telles que \\(x_i\\in\\mathbb R^p\\) et \\(y_i\\in\\mathbb R\\) on rappelle que l’estimateur lasso \\(\\hat\\beta(\\lambda)\\) est construit en minimisant \\[\\begin{equation} \\mathcal L(\\beta)=\\|Y-\\mathbb X\\beta\\|_2^2+\\lambda\\|\\beta\\|_1. \\tag{3.2} \\end{equation}\\] On admettra que la sous-différentielle \\(\\partial \\mathcal L(\\beta)\\) est donnée par \\[\\partial \\mathcal L(\\beta)=\\left\\{-2\\mathbb X^t(Y-\\mathbb X\\beta)+\\lambda z:z\\in\\partial\\|\\beta\\|_1\\right\\}.\\] Montrer que \\(\\hat\\beta(\\lambda)\\) vérifie \\[\\mathbb X^t\\mathbb X\\hat\\beta(\\lambda)=\\mathbb X^tY-\\frac{\\lambda}{2}\\hat z\\] où \\(\\hat z\\in\\mathbb R^p\\) vérifie \\[\\hat z_j\\left\\{ \\begin{array}{ll} =\\textrm{signe}(\\hat\\beta_j(\\lambda)) &amp; \\textrm{si } \\hat\\beta_j(\\lambda)\\neq 0 \\\\ \\in\\left[-1;1\\right] &amp; \\textrm{sinon.} \\end{array}\\right.\\] On suppose maintenant que la matrice \\(\\mathbb X\\) est orthogonale. Montrer que \\[\\textrm{signe}(\\hat\\beta_j(\\lambda))=\\textrm{signe}(\\mathbb X_j^tY)\\quad\\textrm{lorsque }\\hat\\beta_j(\\lambda)\\neq 0\\] et \\(\\hat\\beta_j(\\lambda)=0\\) si et seulement si \\(|\\mathbb X_j^tY|\\leq \\lambda/2\\). En déduire \\[\\hat\\beta_j(\\lambda)=\\mathbb X_j^tY\\left(1-\\frac{\\lambda}{2|\\mathbb X_j^tY|}\\right)_+,\\quad j=1,\\dots,p\\] où \\((x)_+=\\max(x,0)\\). Interpréter ce résultat. Exercice 3.3 (Unicité de l’estimateur lasso,voir Giraud (2015)) Soit \\(\\hat\\beta^{1}(\\lambda)\\) et \\(\\hat\\beta^{2}(\\lambda)\\) deux solutions qui minimisent (3.2). Soit \\(\\hat\\beta=(\\hat\\beta^{1}(\\lambda)+\\hat\\beta^{2}(\\lambda))/2\\). Montrer que si \\(\\mathbb X \\hat\\beta^{1}(\\lambda)\\neq\\mathbb X \\hat\\beta^{2}(\\lambda)\\) alors \\[\\|\\mathbb Y-\\mathbb X\\hat\\beta\\|_2^2+\\lambda\\|\\hat\\beta\\|_1&lt;\\frac{1}{2}\\left(\\|\\mathbb Y-\\mathbb X\\hat\\beta^1(\\lambda)\\|_2^2+\\lambda\\|\\hat\\beta^1(\\lambda)\\|_1+\\|\\mathbb Y-\\mathbb X\\hat\\beta^2(\\lambda)\\|_2^2+\\lambda\\|\\hat\\beta^2(\\lambda)\\|_1\\right).\\] On pourra utiliser la convexité (forte) de \\(x\\mapsto\\|x\\|_2^2\\). En déduire que \\(\\mathbb X \\hat\\beta^{1}(\\lambda)=\\mathbb X \\hat\\beta^{2}(\\lambda)\\). Références "],
["mod-add.html", "Chapitre 4 Modèle additif 4.1 Pseudo backfitting 4.2 Modèle GAM 4.3 Régression logistique additive", " Chapitre 4 Modèle additif Le modèle additif (modèle GAM) peut être vu comme un compromis entre une modélisation linéaire et non paramétrique de la fonction de régression. Il suppose que cette fonction s’écrit \\[m(x)=m(x_1,\\dots,x_d)=\\alpha+g_1(x_1)+\\dots+g_d(x_d).\\] 4.1 Pseudo backfitting L’algorithme du backfitting est souvent utilisé pour estimer les composantes du modèle additif. Etant donné un échantillon \\((x_i,y_i),i=1,\\dots,n\\) on note \\(\\bar{\\mathbb Y}\\) le vecteur des \\(y_i\\) et \\(\\mathbb X_k\\) le vecteur contenant les observations de la variable \\(k\\) pour \\(k=1,\\dots,d\\). L’algorithme se résume ainsi Initialisation : \\(\\widehat\\alpha=\\bar{\\mathbb Y}\\), \\(\\widehat g_k(x_k)=\\bar{\\mathbb X}_k\\). Pour \\(k=1,\\dots,d\\) : \\(\\mathbb Y^{(k)}=\\mathbb Y-\\widehat\\alpha-\\sum_{j\\neq k}\\widehat g_j(\\mathbb X_j)\\) (résidus partiels) \\(\\widehat g_k\\) : lissage non paramétrique de \\(\\mathbb Y^{(k)}\\) sur \\(\\mathbb X_k\\). Répéter l’étape précédente tant que les \\(\\widehat g_k\\) changent. On propose dans cette partie d’utiliser cet algorithme pour estimer les paramètres du modèle linéaire en remplaçant le lissage non paramétrique par un estimateur MCO. On considère le modèle de régression linéaire \\[Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\varepsilon\\] avec \\(X_1\\) et \\(X_2\\) de lois uniformes sur \\([0,1]\\) et \\(\\varepsilon\\) de loi \\(\\mathcal N(0,1)\\) (\\(\\varepsilon\\) est indépendante de \\((X_1,X_2)^\\prime\\)). Générer un échantillon \\((x_i,y_i)\\) de taille \\(n=300\\) selon le modèle ci-dessus pour \\(\\beta_0=1,\\beta_1=3,\\beta_2=5\\). Créer une fonction R qui admet en entrée un jeu de données et qui fournit en sortie les estimateurs par la méthode du backfitting. En déduire les estimateurs backfitting pour le problème considéré. Comparer aux estimateurs MCO. 4.2 Modèle GAM On considère les données générées selon n &lt;- 1000 set.seed(1465) X1 &lt;- 2*runif(n) X2 &lt;- 2*runif(n) bruit &lt;- rnorm(n) Y &lt;- 2*X1+sin(8*pi*X2)+bruit donnees&lt;-data.frame(Y,X1,X2) Écrire le modèle A l’aide du package gam visualiser les estimateurs des composantes additives du modèle. On utilisera tout d’abord un lissage par spline avec 1 ddl pour la première composante et 24.579 ddl pour la seconde. Faire varier les degrés de liberté, interpréter. Faire le même travail avec le lisseur loess. On commencera avec degree=2 et span=0.15 puis on fera varier le paramètre span. Estimer le degrés de liberté avec la fonction gam du package mgcv (Il n’est pas nécessaire de charger le package pour éviter les conflits). 4.3 Régression logistique additive On considère le jeu de données panne.txt qui recense des pannes de machine (etat=1) en fonction de leur âge et de leur marque. Faire une régression logistique permettant d’expliquer la variable etat par la variable age uniquement. Critiquer le modèle. Ajuster un modèle additif, toujours avec uniquement la variable age. En utilisant le modèle additif, proposer un nouveau modèle logistique plus pertinent. "],
["références.html", "Références", " Références "]
]
